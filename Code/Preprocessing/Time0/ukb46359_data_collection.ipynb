{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7bda9b3",
   "metadata": {},
   "source": [
    "# Data collection and cleaning: ukb46359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d32046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the json module, which allows us to work with JSON files in Python.\n",
    "import json\n",
    "with open('/Users/marinacamacho/Desktop/Master_I/var.json') as f:\n",
    "    var_temp = json.load(f)\n",
    "\n",
    "# Import the numpy module. Numpy is a library in Python that provides support for large, \n",
    "# multi-dimensional arrays and matrices, along with a large collection of high-level \n",
    "# mathematical functions to operate on these arrays.\n",
    "import numpy as np\n",
    "\n",
    "# Import the pandas module, which allows us to work with data structures and data analysis tools.\n",
    "# Given it's a large dataset, the 'nrows=1' argument is used to read only the first row of the CSV file.\n",
    "import pandas as pd\n",
    "df_ = pd.read_csv('/Users/marinacamacho/Desktop/Master_I/Raw_Data/ukb46359.csv', nrows=1)  # Read only the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1aaf12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20008-1.9',\n",
       " '20008-1.10',\n",
       " '20008-1.11',\n",
       " '20008-1.12',\n",
       " '20008-1.13',\n",
       " '20008-1.14',\n",
       " '20008-1.15',\n",
       " '20008-1.16',\n",
       " '20008-1.17',\n",
       " '20008-1.18',\n",
       " '20008-1.19',\n",
       " '20008-1.20',\n",
       " '20008-1.21',\n",
       " '20008-1.22',\n",
       " '20008-1.23',\n",
       " '20008-1.24',\n",
       " '20008-1.25',\n",
       " '20008-1.26',\n",
       " '20008-1.27',\n",
       " '20008-1.28',\n",
       " '20008-1.29',\n",
       " '20008-1.30',\n",
       " '20008-1.31',\n",
       " '20008-1.32',\n",
       " '20008-1.33',\n",
       " '20008-2.0',\n",
       " '20008-2.1',\n",
       " '20008-2.2',\n",
       " '20008-2.3',\n",
       " '20008-2.4',\n",
       " '20008-2.5',\n",
       " '20008-2.6',\n",
       " '20008-2.7',\n",
       " '20008-2.8',\n",
       " '20008-2.9',\n",
       " '20008-2.10',\n",
       " '20008-2.11',\n",
       " '20008-2.12',\n",
       " '20008-2.13',\n",
       " '20008-2.14',\n",
       " '20008-2.15',\n",
       " '20008-2.16',\n",
       " '20008-2.17',\n",
       " '20008-2.18',\n",
       " '20008-2.19',\n",
       " '20008-2.20',\n",
       " '20008-2.21',\n",
       " '20008-2.22',\n",
       " '20008-2.23',\n",
       " '20008-2.24',\n",
       " '20008-2.25',\n",
       " '20008-2.26',\n",
       " '20008-2.27',\n",
       " '20008-2.28',\n",
       " '20008-2.29',\n",
       " '20008-2.30',\n",
       " '20008-2.31',\n",
       " '20008-2.32',\n",
       " '20008-2.33',\n",
       " '20008-3.0',\n",
       " '20008-3.1',\n",
       " '20008-3.2',\n",
       " '20008-3.3',\n",
       " '20008-3.4',\n",
       " '20008-3.5',\n",
       " '20008-3.6',\n",
       " '20008-3.7',\n",
       " '20008-3.8',\n",
       " '20008-3.9',\n",
       " '20008-3.10',\n",
       " '20008-3.11',\n",
       " '20008-3.12',\n",
       " '20008-3.13',\n",
       " '20008-3.14',\n",
       " '20008-3.15',\n",
       " '20008-3.16',\n",
       " '20008-3.17',\n",
       " '20008-3.18',\n",
       " '20008-3.19',\n",
       " '20008-3.20',\n",
       " '20008-3.21',\n",
       " '20008-3.22',\n",
       " '20008-3.23',\n",
       " '20008-3.24',\n",
       " '20008-3.25',\n",
       " '20008-3.26',\n",
       " '20008-3.27',\n",
       " '20008-3.28',\n",
       " '20008-3.29',\n",
       " '20008-3.30',\n",
       " '20008-3.31',\n",
       " '20008-3.32',\n",
       " '20008-3.33',\n",
       " '20009-0.0',\n",
       " '20009-0.1',\n",
       " '20009-0.2',\n",
       " '20009-0.3',\n",
       " '20009-0.4',\n",
       " '20009-0.5',\n",
       " '20009-0.6',\n",
       " '20009-0.7',\n",
       " '20009-0.8',\n",
       " '20009-0.9',\n",
       " '20009-0.10',\n",
       " '20009-0.11',\n",
       " '20009-0.12',\n",
       " '20009-0.13',\n",
       " '20009-0.14',\n",
       " '20009-0.15',\n",
       " '20009-0.16',\n",
       " '20009-0.17',\n",
       " '20009-0.18',\n",
       " '20009-0.19',\n",
       " '20009-0.20',\n",
       " '20009-0.21',\n",
       " '20009-0.22',\n",
       " '20009-0.23',\n",
       " '20009-0.24',\n",
       " '20009-0.25',\n",
       " '20009-0.26',\n",
       " '20009-0.27',\n",
       " '20009-0.28',\n",
       " '20009-0.29',\n",
       " '20009-0.30',\n",
       " '20009-0.31',\n",
       " '20009-0.32',\n",
       " '20009-0.33',\n",
       " '20009-1.0',\n",
       " '20009-1.1',\n",
       " '20009-1.2',\n",
       " '20009-1.3',\n",
       " '20009-1.4',\n",
       " '20009-1.5',\n",
       " '20009-1.6',\n",
       " '20009-1.7',\n",
       " '20009-1.8',\n",
       " '20009-1.9',\n",
       " '20009-1.10',\n",
       " '20009-1.11',\n",
       " '20009-1.12',\n",
       " '20009-1.13',\n",
       " '20009-1.14',\n",
       " '20009-1.15',\n",
       " '20009-1.16',\n",
       " '20009-1.17',\n",
       " '20009-1.18',\n",
       " '20009-1.19',\n",
       " '20009-1.20',\n",
       " '20009-1.21',\n",
       " '20009-1.22',\n",
       " '20009-1.23',\n",
       " '20009-1.24',\n",
       " '20009-1.25',\n",
       " '20009-1.26',\n",
       " '20009-1.27',\n",
       " '20009-1.28',\n",
       " '20009-1.29',\n",
       " '20009-1.30',\n",
       " '20009-1.31',\n",
       " '20009-1.32',\n",
       " '20009-1.33',\n",
       " '20009-2.0',\n",
       " '20009-2.1',\n",
       " '20009-2.2',\n",
       " '20009-2.3',\n",
       " '20009-2.4',\n",
       " '20009-2.5',\n",
       " '20009-2.6',\n",
       " '20009-2.7',\n",
       " '20009-2.8',\n",
       " '20009-2.9',\n",
       " '20009-2.10',\n",
       " '20009-2.11',\n",
       " '20009-2.12',\n",
       " '20009-2.13',\n",
       " '20009-2.14',\n",
       " '20009-2.15',\n",
       " '20009-2.16',\n",
       " '20009-2.17',\n",
       " '20009-2.18',\n",
       " '20009-2.19',\n",
       " '20009-2.20',\n",
       " '20009-2.21',\n",
       " '20009-2.22',\n",
       " '20009-2.23',\n",
       " '20009-2.24',\n",
       " '20009-2.25',\n",
       " '20009-2.26',\n",
       " '20009-2.27',\n",
       " '20009-2.28',\n",
       " '20009-2.29',\n",
       " '20009-2.30',\n",
       " '20009-2.31',\n",
       " '20009-2.32',\n",
       " '20009-2.33',\n",
       " '20009-3.0',\n",
       " '20009-3.1',\n",
       " '20009-3.2',\n",
       " '20009-3.3',\n",
       " '20009-3.4',\n",
       " '20009-3.5',\n",
       " '20009-3.6',\n",
       " '20009-3.7',\n",
       " '20009-3.8',\n",
       " '20009-3.9',\n",
       " '20009-3.10',\n",
       " '20009-3.11',\n",
       " '20009-3.12',\n",
       " '20009-3.13',\n",
       " '20009-3.14',\n",
       " '20009-3.15',\n",
       " '20009-3.16',\n",
       " '20009-3.17',\n",
       " '20009-3.18',\n",
       " '20009-3.19',\n",
       " '20009-3.20',\n",
       " '20009-3.21',\n",
       " '20009-3.22',\n",
       " '20009-3.23',\n",
       " '20009-3.24',\n",
       " '20009-3.25',\n",
       " '20009-3.26',\n",
       " '20009-3.27',\n",
       " '20009-3.28',\n",
       " '20009-3.29',\n",
       " '20009-3.30',\n",
       " '20009-3.31',\n",
       " '20009-3.32',\n",
       " '20009-3.33',\n",
       " '20012-0.0',\n",
       " '20012-0.1',\n",
       " '20012-0.2',\n",
       " '20012-0.3',\n",
       " '20012-0.4',\n",
       " '20012-0.5',\n",
       " '20012-1.0',\n",
       " '20012-1.1',\n",
       " '20012-1.2',\n",
       " '20012-1.3',\n",
       " '20012-1.4',\n",
       " '20012-1.5',\n",
       " '20012-2.0',\n",
       " '20012-2.1',\n",
       " '20012-2.2',\n",
       " '20012-2.3',\n",
       " '20012-2.4',\n",
       " '20012-2.5',\n",
       " '20012-3.0',\n",
       " '20012-3.1',\n",
       " '20012-3.2',\n",
       " '20012-3.3',\n",
       " '20012-3.4',\n",
       " '20012-3.5',\n",
       " '20086-0.0',\n",
       " '20086-0.1',\n",
       " '20086-0.2',\n",
       " '20086-0.3',\n",
       " '20086-0.4',\n",
       " '20086-0.5',\n",
       " '20086-1.0',\n",
       " '20086-1.1',\n",
       " '20086-1.2',\n",
       " '20086-1.3',\n",
       " '20086-1.4',\n",
       " '20086-1.5',\n",
       " '20086-2.0',\n",
       " '20086-2.1',\n",
       " '20086-2.2',\n",
       " '20086-2.3',\n",
       " '20086-2.4',\n",
       " '20086-2.5',\n",
       " '20086-3.0',\n",
       " '20086-3.1',\n",
       " '20086-3.2',\n",
       " '20086-3.3',\n",
       " '20086-3.4',\n",
       " '20086-3.5',\n",
       " '20086-4.0',\n",
       " '20086-4.1',\n",
       " '20086-4.2',\n",
       " '20086-4.3',\n",
       " '20086-4.4',\n",
       " '20086-4.5',\n",
       " '20116-0.0',\n",
       " '20116-1.0',\n",
       " '20116-2.0',\n",
       " '20116-3.0',\n",
       " '20117-0.0',\n",
       " '20117-1.0',\n",
       " '20117-2.0',\n",
       " '20117-3.0',\n",
       " '20119-0.0',\n",
       " '20123-0.0',\n",
       " '20124-0.0',\n",
       " '20125-0.0',\n",
       " '20126-0.0',\n",
       " '20127-0.0',\n",
       " '20252-2.0',\n",
       " '20252-3.0',\n",
       " '20253-2.0',\n",
       " '20253-3.0',\n",
       " '20400-0.0',\n",
       " '20403-0.0',\n",
       " '20405-0.0',\n",
       " '20407-0.0',\n",
       " '20408-0.0',\n",
       " '20409-0.0',\n",
       " '20410-0.0',\n",
       " '20411-0.0',\n",
       " '20412-0.0',\n",
       " '20413-0.0',\n",
       " '20414-0.0',\n",
       " '20416-0.0',\n",
       " '20417-0.0',\n",
       " '20418-0.0',\n",
       " '20419-0.0',\n",
       " '20420-0.0',\n",
       " '20421-0.0',\n",
       " '20422-0.0',\n",
       " '20423-0.0',\n",
       " '20425-0.0',\n",
       " '20426-0.0',\n",
       " '20427-0.0',\n",
       " '20428-0.0',\n",
       " '20429-0.0',\n",
       " '20433-0.0',\n",
       " '20434-0.0',\n",
       " '20435-0.0',\n",
       " '20436-0.0',\n",
       " '20437-0.0',\n",
       " '20438-0.0',\n",
       " '20439-0.0',\n",
       " '20440-0.0',\n",
       " '20441-0.0',\n",
       " '20442-0.0',\n",
       " '20445-0.0',\n",
       " '20446-0.0',\n",
       " '20447-0.0',\n",
       " '20448-0.0',\n",
       " '20449-0.0',\n",
       " '20450-0.0',\n",
       " '20453-0.0',\n",
       " '20454-0.0',\n",
       " '20455-0.0',\n",
       " '20456-0.0',\n",
       " '20461-0.0',\n",
       " '20462-0.0',\n",
       " '20463-0.0',\n",
       " '20465-0.0',\n",
       " '20466-0.0',\n",
       " '20467-0.0',\n",
       " '20468-0.0',\n",
       " '20470-0.0',\n",
       " '20471-0.0',\n",
       " '20473-0.0',\n",
       " '20474-0.0',\n",
       " '20476-0.0',\n",
       " '20477-0.0',\n",
       " '20487-0.0',\n",
       " '20488-0.0',\n",
       " '20489-0.0',\n",
       " '20490-0.0',\n",
       " '20491-0.0',\n",
       " '20492-0.0',\n",
       " '20493-0.0',\n",
       " '20494-0.0',\n",
       " '20495-0.0',\n",
       " '20496-0.0',\n",
       " '20497-0.0',\n",
       " '20498-0.0',\n",
       " '20499-0.0',\n",
       " '20500-0.0',\n",
       " '20501-0.0',\n",
       " '20502-0.0',\n",
       " '20505-0.0',\n",
       " '20506-0.0',\n",
       " '20507-0.0',\n",
       " '20508-0.0',\n",
       " '20509-0.0',\n",
       " '20510-0.0',\n",
       " '20511-0.0',\n",
       " '20512-0.0',\n",
       " '20513-0.0',\n",
       " '20514-0.0',\n",
       " '20515-0.0',\n",
       " '20516-0.0',\n",
       " '20517-0.0',\n",
       " '20518-0.0',\n",
       " '20519-0.0',\n",
       " '20520-0.0',\n",
       " '20521-0.0',\n",
       " '20522-0.0',\n",
       " '20523-0.0',\n",
       " '20524-0.0',\n",
       " '20525-0.0',\n",
       " '20526-0.0',\n",
       " '20527-0.0',\n",
       " '20528-0.0',\n",
       " '20529-0.0',\n",
       " '20530-0.0',\n",
       " '20531-0.0',\n",
       " '20532-0.0',\n",
       " '20533-0.0',\n",
       " '20534-0.0',\n",
       " '20535-0.0',\n",
       " '20536-0.0',\n",
       " '20537-0.0',\n",
       " '20538-0.0',\n",
       " '20539-0.0',\n",
       " '20540-0.0',\n",
       " '20541-0.0',\n",
       " '20542-0.0',\n",
       " '20543-0.0',\n",
       " '20544-0.1',\n",
       " '20544-0.2',\n",
       " '20544-0.3',\n",
       " '20544-0.4',\n",
       " '20544-0.5',\n",
       " '20544-0.6',\n",
       " '20544-0.7',\n",
       " '20544-0.8',\n",
       " '20544-0.9',\n",
       " '20544-0.10',\n",
       " '20544-0.11',\n",
       " '20544-0.12',\n",
       " '20544-0.13',\n",
       " '20544-0.14',\n",
       " '20544-0.15',\n",
       " '20544-0.16',\n",
       " '20546-0.1',\n",
       " '20546-0.2',\n",
       " '20546-0.3',\n",
       " '20547-0.1',\n",
       " '20547-0.2',\n",
       " '20548-0.1',\n",
       " '20548-0.2',\n",
       " '20548-0.3',\n",
       " '20548-0.4',\n",
       " '20548-0.5',\n",
       " '20548-0.6',\n",
       " '20548-0.7',\n",
       " '20548-0.8',\n",
       " '20549-0.1',\n",
       " '20549-0.2',\n",
       " '20549-0.3',\n",
       " '20550-0.1',\n",
       " '20550-0.2',\n",
       " '21000-0.0',\n",
       " '21000-1.0',\n",
       " '21000-2.0',\n",
       " '21001-0.0',\n",
       " '21001-1.0',\n",
       " '21001-2.0',\n",
       " '21001-3.0',\n",
       " '21003-0.0',\n",
       " '21003-1.0',\n",
       " '21003-2.0',\n",
       " '21003-3.0',\n",
       " '21022-0.0',\n",
       " '22006-0.0',\n",
       " '22032-0.0',\n",
       " '22033-0.0',\n",
       " '22034-0.0',\n",
       " '22035-0.0',\n",
       " '22036-0.0',\n",
       " '22037-0.0',\n",
       " '22038-0.0',\n",
       " '22039-0.0',\n",
       " '23098-0.0',\n",
       " '23098-1.0',\n",
       " '23098-2.0',\n",
       " '23098-3.0',\n",
       " '23099-0.0',\n",
       " '23099-1.0',\n",
       " '23099-2.0',\n",
       " '23099-3.0',\n",
       " '23100-0.0',\n",
       " '23100-1.0',\n",
       " '23100-2.0',\n",
       " '23100-3.0',\n",
       " '23101-0.0',\n",
       " '23101-1.0',\n",
       " '23101-2.0',\n",
       " '23101-3.0',\n",
       " '23245-2.0',\n",
       " '25056-2.0',\n",
       " '25056-3.0',\n",
       " '25057-2.0',\n",
       " '25057-3.0',\n",
       " '25058-2.0',\n",
       " '25058-3.0',\n",
       " '25059-2.0',\n",
       " '25059-3.0',\n",
       " '25060-2.0',\n",
       " '25060-3.0',\n",
       " '25061-2.0',\n",
       " '25061-3.0',\n",
       " '25062-2.0',\n",
       " '25062-3.0',\n",
       " '25063-2.0',\n",
       " '25063-3.0',\n",
       " '25064-2.0',\n",
       " '25064-3.0',\n",
       " '25065-2.0',\n",
       " '25065-3.0',\n",
       " '25066-2.0',\n",
       " '25066-3.0',\n",
       " '25067-2.0',\n",
       " '25067-3.0',\n",
       " '25068-2.0',\n",
       " '25068-3.0',\n",
       " '25069-2.0',\n",
       " '25069-3.0',\n",
       " '25070-2.0',\n",
       " '25070-3.0',\n",
       " '25071-2.0',\n",
       " '25071-3.0',\n",
       " '25072-2.0',\n",
       " '25072-3.0',\n",
       " '25073-2.0',\n",
       " '25073-3.0',\n",
       " '25074-2.0',\n",
       " '25074-3.0',\n",
       " '25075-2.0',\n",
       " '25075-3.0',\n",
       " '25076-2.0',\n",
       " '25076-3.0',\n",
       " '25077-2.0',\n",
       " '25077-3.0',\n",
       " '25078-2.0',\n",
       " '25078-3.0',\n",
       " '25079-2.0',\n",
       " '25079-3.0',\n",
       " '25080-2.0',\n",
       " '25080-3.0',\n",
       " '25081-2.0',\n",
       " '25081-3.0',\n",
       " '25082-2.0',\n",
       " '25082-3.0',\n",
       " '25083-2.0',\n",
       " '25083-3.0',\n",
       " '25084-2.0',\n",
       " '25084-3.0',\n",
       " '25085-2.0',\n",
       " '25085-3.0',\n",
       " '25086-2.0',\n",
       " '25086-3.0',\n",
       " '25087-2.0',\n",
       " '25087-3.0',\n",
       " '25088-2.0',\n",
       " '25088-3.0',\n",
       " '25089-2.0',\n",
       " '25089-3.0',\n",
       " '25090-2.0',\n",
       " '25090-3.0',\n",
       " '25091-2.0',\n",
       " '25091-3.0',\n",
       " '25092-2.0',\n",
       " '25092-3.0',\n",
       " '25093-2.0',\n",
       " '25093-3.0',\n",
       " '25094-2.0',\n",
       " '25094-3.0',\n",
       " '25095-2.0',\n",
       " '25095-3.0',\n",
       " '25096-2.0',\n",
       " '25096-3.0',\n",
       " '25097-2.0',\n",
       " '25097-3.0',\n",
       " '25098-2.0',\n",
       " '25098-3.0',\n",
       " '25099-2.0',\n",
       " '25099-3.0',\n",
       " '25100-2.0',\n",
       " '25100-3.0',\n",
       " '25101-2.0',\n",
       " '25101-3.0',\n",
       " '25102-2.0',\n",
       " '25102-3.0',\n",
       " '25103-2.0',\n",
       " '25103-3.0',\n",
       " '25104-2.0',\n",
       " '25104-3.0',\n",
       " '25105-2.0',\n",
       " '25105-3.0',\n",
       " '25106-2.0',\n",
       " '25106-3.0',\n",
       " '25107-2.0',\n",
       " '25107-3.0',\n",
       " '25108-2.0',\n",
       " '25108-3.0',\n",
       " '25109-2.0',\n",
       " '25109-3.0',\n",
       " '25110-2.0',\n",
       " '25110-3.0',\n",
       " '25111-2.0',\n",
       " '25111-3.0',\n",
       " '25112-2.0',\n",
       " '25112-3.0',\n",
       " '25113-2.0',\n",
       " '25113-3.0',\n",
       " '25114-2.0',\n",
       " '25114-3.0',\n",
       " '25115-2.0',\n",
       " '25115-3.0',\n",
       " '25116-2.0',\n",
       " '25116-3.0',\n",
       " '25117-2.0',\n",
       " '25117-3.0',\n",
       " '25118-2.0',\n",
       " '25118-3.0',\n",
       " '25119-2.0',\n",
       " '25119-3.0',\n",
       " '25120-2.0',\n",
       " '25120-3.0',\n",
       " '25121-2.0',\n",
       " '25121-3.0',\n",
       " '25122-2.0',\n",
       " '25122-3.0',\n",
       " '25123-2.0',\n",
       " '25123-3.0',\n",
       " '25124-2.0',\n",
       " '25124-3.0',\n",
       " '25125-2.0',\n",
       " '25125-3.0',\n",
       " '25126-2.0',\n",
       " '25126-3.0',\n",
       " '25127-2.0',\n",
       " '25127-3.0',\n",
       " '25128-2.0',\n",
       " '25128-3.0',\n",
       " '25129-2.0',\n",
       " '25129-3.0',\n",
       " '25130-2.0',\n",
       " '25130-3.0',\n",
       " '25131-2.0',\n",
       " '25131-3.0',\n",
       " '25132-2.0',\n",
       " '25132-3.0',\n",
       " '25133-2.0',\n",
       " '25133-3.0',\n",
       " '25134-2.0',\n",
       " '25134-3.0',\n",
       " '25135-2.0',\n",
       " '25135-3.0',\n",
       " '25136-2.0',\n",
       " '25136-3.0',\n",
       " '25137-2.0',\n",
       " '25137-3.0',\n",
       " '25138-2.0',\n",
       " '25138-3.0',\n",
       " '25139-2.0',\n",
       " '25139-3.0',\n",
       " '25140-2.0',\n",
       " '25140-3.0',\n",
       " '25141-2.0',\n",
       " '25141-3.0',\n",
       " '25142-2.0',\n",
       " '25142-3.0',\n",
       " '25143-2.0',\n",
       " '25143-3.0',\n",
       " '25144-2.0',\n",
       " '25144-3.0',\n",
       " '25145-2.0',\n",
       " '25145-3.0',\n",
       " '25146-2.0',\n",
       " '25146-3.0',\n",
       " '25147-2.0',\n",
       " '25147-3.0',\n",
       " '25148-2.0',\n",
       " '25148-3.0',\n",
       " '25149-2.0',\n",
       " '25149-3.0',\n",
       " '25150-2.0',\n",
       " '25150-3.0',\n",
       " '25151-2.0',\n",
       " '25151-3.0',\n",
       " '25488-2.0',\n",
       " '25488-3.0',\n",
       " '25489-2.0',\n",
       " '25489-3.0',\n",
       " '25490-2.0',\n",
       " '25490-3.0',\n",
       " '25491-2.0',\n",
       " '25491-3.0',\n",
       " '25492-2.0',\n",
       " '25492-3.0',\n",
       " '25493-2.0',\n",
       " '25493-3.0',\n",
       " '25494-2.0',\n",
       " '25494-3.0',\n",
       " '25495-2.0',\n",
       " '25495-3.0',\n",
       " '25496-2.0',\n",
       " '25496-3.0',\n",
       " '25497-2.0',\n",
       " '25497-3.0',\n",
       " '25498-2.0',\n",
       " '25498-3.0',\n",
       " '25499-2.0',\n",
       " '25499-3.0',\n",
       " '25500-2.0',\n",
       " '25500-3.0',\n",
       " '25501-2.0',\n",
       " '25501-3.0',\n",
       " '25502-2.0',\n",
       " '25502-3.0',\n",
       " '25503-2.0',\n",
       " '25503-3.0',\n",
       " '25504-2.0',\n",
       " '25504-3.0',\n",
       " '25505-2.0',\n",
       " '25505-3.0',\n",
       " '25506-2.0',\n",
       " '25506-3.0',\n",
       " '25507-2.0',\n",
       " '25507-3.0',\n",
       " '25508-2.0',\n",
       " '25508-3.0',\n",
       " '25509-2.0',\n",
       " '25509-3.0',\n",
       " '25510-2.0',\n",
       " '25510-3.0',\n",
       " '25511-2.0',\n",
       " '25511-3.0',\n",
       " '25512-2.0',\n",
       " '25512-3.0',\n",
       " '25513-2.0',\n",
       " '25513-3.0',\n",
       " '25514-2.0',\n",
       " '25514-3.0',\n",
       " '25515-2.0',\n",
       " '25515-3.0',\n",
       " '25516-2.0',\n",
       " '25516-3.0',\n",
       " '25517-2.0',\n",
       " '25517-3.0',\n",
       " '25518-2.0',\n",
       " '25518-3.0',\n",
       " '25519-2.0',\n",
       " '25519-3.0',\n",
       " '25520-2.0',\n",
       " '25520-3.0',\n",
       " '25521-2.0',\n",
       " '25521-3.0',\n",
       " '25522-2.0',\n",
       " '25522-3.0',\n",
       " '25523-2.0',\n",
       " '25523-3.0',\n",
       " '25524-2.0',\n",
       " '25524-3.0',\n",
       " '25525-2.0',\n",
       " '25525-3.0',\n",
       " '25526-2.0',\n",
       " '25526-3.0',\n",
       " '25527-2.0',\n",
       " '25527-3.0',\n",
       " '25528-2.0',\n",
       " '25528-3.0',\n",
       " '25529-2.0',\n",
       " '25529-3.0',\n",
       " '25530-2.0',\n",
       " '25530-3.0',\n",
       " '25531-2.0',\n",
       " '25531-3.0',\n",
       " '25532-2.0',\n",
       " '25532-3.0',\n",
       " '25533-2.0',\n",
       " '25533-3.0',\n",
       " '25534-2.0',\n",
       " '25534-3.0',\n",
       " '25535-2.0',\n",
       " '25535-3.0',\n",
       " '25536-2.0',\n",
       " '25536-3.0',\n",
       " '25537-2.0',\n",
       " '25537-3.0',\n",
       " '25538-2.0',\n",
       " '25538-3.0',\n",
       " '25539-2.0',\n",
       " '25539-3.0',\n",
       " '25540-2.0',\n",
       " '25540-3.0',\n",
       " '25541-2.0',\n",
       " '25541-3.0',\n",
       " '25736-2.0',\n",
       " '25736-3.0',\n",
       " '25781-2.0',\n",
       " '25781-3.0',\n",
       " '26514-2.0',\n",
       " '26514-3.0',\n",
       " '26515-2.0',\n",
       " '26515-3.0',\n",
       " '26516-2.0',\n",
       " '26516-3.0',\n",
       " '26517-2.0',\n",
       " '26517-3.0',\n",
       " '26518-2.0',\n",
       " '26518-3.0',\n",
       " '26519-2.0',\n",
       " '26519-3.0',\n",
       " '26520-2.0',\n",
       " '26520-3.0',\n",
       " '26521-2.0',\n",
       " '26521-3.0',\n",
       " '26522-2.0',\n",
       " '26522-3.0',\n",
       " '26526-2.0',\n",
       " '26526-3.0',\n",
       " '26527-2.0',\n",
       " '26527-3.0',\n",
       " '26528-2.0',\n",
       " '26528-3.0',\n",
       " '26529-2.0',\n",
       " '26529-3.0',\n",
       " '26530-2.0',\n",
       " '26530-3.0',\n",
       " '26531-2.0',\n",
       " '26531-3.0',\n",
       " '26532-2.0',\n",
       " '26532-3.0',\n",
       " '26533-2.0',\n",
       " '26533-3.0',\n",
       " '26534-2.0',\n",
       " '26534-3.0',\n",
       " '26535-2.0',\n",
       " '26535-3.0',\n",
       " '26536-2.0',\n",
       " '26536-3.0',\n",
       " '26537-2.0',\n",
       " '26537-3.0',\n",
       " '26552-2.0',\n",
       " '26552-3.0',\n",
       " '26553-2.0',\n",
       " '26553-3.0',\n",
       " '26554-2.0',\n",
       " '26554-3.0',\n",
       " '26555-2.0',\n",
       " '26555-3.0',\n",
       " '26556-2.0',\n",
       " '26556-3.0',\n",
       " '26557-2.0',\n",
       " '26557-3.0',\n",
       " '26558-2.0',\n",
       " '26558-3.0',\n",
       " '26559-2.0',\n",
       " '26559-3.0',\n",
       " '26560-2.0',\n",
       " '26560-3.0',\n",
       " '26561-2.0',\n",
       " '26561-3.0',\n",
       " '26562-2.0',\n",
       " '26562-3.0',\n",
       " '26563-2.0',\n",
       " '26563-3.0',\n",
       " '26564-2.0',\n",
       " '26564-3.0',\n",
       " '26565-2.0',\n",
       " '26565-3.0',\n",
       " '26566-2.0',\n",
       " '26566-3.0',\n",
       " '26567-2.0',\n",
       " '26567-3.0',\n",
       " '26583-2.0',\n",
       " '26583-3.0',\n",
       " '26584-2.0',\n",
       " '26584-3.0',\n",
       " '26585-2.0',\n",
       " '26585-3.0',\n",
       " '26586-2.0',\n",
       " '26586-3.0',\n",
       " '26587-2.0',\n",
       " '26587-3.0',\n",
       " '26588-2.0',\n",
       " '26588-3.0',\n",
       " '26589-2.0',\n",
       " '26589-3.0',\n",
       " '26590-2.0',\n",
       " '26590-3.0',\n",
       " '26591-2.0',\n",
       " '26591-3.0',\n",
       " '26592-2.0',\n",
       " '26592-3.0',\n",
       " '26593-2.0',\n",
       " '26593-3.0',\n",
       " '26594-2.0',\n",
       " '26594-3.0',\n",
       " '26595-2.0',\n",
       " '26595-3.0',\n",
       " '26596-2.0',\n",
       " '26596-3.0',\n",
       " '26597-2.0',\n",
       " '26597-3.0',\n",
       " '26598-2.0',\n",
       " '26598-3.0',\n",
       " '26721-2.0',\n",
       " '26721-3.0',\n",
       " '26722-2.0',\n",
       " '26722-3.0',\n",
       " '26723-2.0',\n",
       " '26723-3.0',\n",
       " '26724-2.0',\n",
       " '26724-3.0',\n",
       " '26725-2.0',\n",
       " '26725-3.0',\n",
       " '26726-2.0',\n",
       " '26726-3.0',\n",
       " '26727-2.0',\n",
       " '26727-3.0',\n",
       " '26728-2.0',\n",
       " '26728-3.0',\n",
       " '26729-2.0',\n",
       " '26729-3.0',\n",
       " '26730-2.0',\n",
       " '26730-3.0',\n",
       " '26731-2.0',\n",
       " '26731-3.0',\n",
       " '26732-2.0',\n",
       " '26732-3.0',\n",
       " '26733-2.0',\n",
       " '26733-3.0',\n",
       " '26734-2.0',\n",
       " '26734-3.0',\n",
       " '26735-2.0',\n",
       " '26735-3.0',\n",
       " '26736-2.0',\n",
       " '26736-3.0',\n",
       " '26737-2.0',\n",
       " '26737-3.0',\n",
       " '26738-2.0',\n",
       " '26738-3.0',\n",
       " '26739-2.0',\n",
       " '26739-3.0',\n",
       " '26740-2.0',\n",
       " '26740-3.0',\n",
       " '26741-2.0',\n",
       " '26741-3.0',\n",
       " '26742-2.0',\n",
       " '26742-3.0',\n",
       " '26743-2.0',\n",
       " '26743-3.0',\n",
       " '26744-2.0',\n",
       " '26744-3.0',\n",
       " '26745-2.0',\n",
       " '26745-3.0',\n",
       " '26746-2.0',\n",
       " '26746-3.0',\n",
       " '26747-2.0',\n",
       " '26747-3.0',\n",
       " '26748-2.0',\n",
       " '26748-3.0',\n",
       " '26749-2.0',\n",
       " '26749-3.0',\n",
       " '26750-2.0',\n",
       " '26750-3.0',\n",
       " '26751-2.0',\n",
       " '26751-3.0',\n",
       " '26752-2.0',\n",
       " '26752-3.0',\n",
       " '26753-2.0',\n",
       " '26753-3.0',\n",
       " '26754-2.0',\n",
       " '26754-3.0',\n",
       " '26755-2.0',\n",
       " '26755-3.0',\n",
       " '26756-2.0',\n",
       " '26756-3.0',\n",
       " '26757-2.0',\n",
       " '26757-3.0',\n",
       " '26758-2.0',\n",
       " '26758-3.0',\n",
       " '26759-2.0',\n",
       " '26759-3.0',\n",
       " '26760-2.0',\n",
       " '26760-3.0',\n",
       " '26761-2.0',\n",
       " '26761-3.0',\n",
       " '26762-2.0',\n",
       " '26762-3.0',\n",
       " '26763-2.0',\n",
       " '26763-3.0',\n",
       " '26764-2.0',\n",
       " '26764-3.0',\n",
       " '26765-2.0',\n",
       " '26765-3.0',\n",
       " '26766-2.0',\n",
       " '26766-3.0',\n",
       " '26767-2.0',\n",
       " '26767-3.0',\n",
       " '26768-2.0',\n",
       " '26768-3.0',\n",
       " '26769-2.0',\n",
       " '26769-3.0',\n",
       " '26770-2.0',\n",
       " '26770-3.0',\n",
       " '26771-2.0',\n",
       " '26771-3.0',\n",
       " '26772-2.0',\n",
       " '26772-3.0',\n",
       " '26773-2.0',\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_.columns)[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64e329ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_including_minus_0 = df_.filter(regex='.*-0.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6ca06d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>31-0.0</th>\n",
       "      <th>44-0.0</th>\n",
       "      <th>48-0.0</th>\n",
       "      <th>49-0.0</th>\n",
       "      <th>53-0.0</th>\n",
       "      <th>54-0.0</th>\n",
       "      <th>55-0.0</th>\n",
       "      <th>74-0.0</th>\n",
       "      <th>87-0.0</th>\n",
       "      <th>87-0.1</th>\n",
       "      <th>...</th>\n",
       "      <th>131414-0.0</th>\n",
       "      <th>131415-0.0</th>\n",
       "      <th>131416-0.0</th>\n",
       "      <th>131417-0.0</th>\n",
       "      <th>131418-0.0</th>\n",
       "      <th>131419-0.0</th>\n",
       "      <th>131420-0.0</th>\n",
       "      <th>131421-0.0</th>\n",
       "      <th>131422-0.0</th>\n",
       "      <th>131423-0.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4675</td>\n",
       "      <td>74</td>\n",
       "      <td>102</td>\n",
       "      <td>2010-02-25</td>\n",
       "      <td>11018</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 1677 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   31-0.0  44-0.0  48-0.0  49-0.0      53-0.0  54-0.0  55-0.0  74-0.0  87-0.0  \\\n",
       "0       0    4675      74     102  2010-02-25   11018       2       4      40   \n",
       "\n",
       "   87-0.1  ...  131414-0.0  131415-0.0  131416-0.0  131417-0.0  131418-0.0  \\\n",
       "0      44  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "   131419-0.0  131420-0.0  131421-0.0  131422-0.0  131423-0.0  \n",
       "0         NaN         NaN         NaN         NaN         NaN  \n",
       "\n",
       "[1 rows x 1677 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_including_minus_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a57d8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1677"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(columns_including_minus_0.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b98c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variables that we had in a previous version that are no longer available\n",
    "not_found_1 = ['767-0.0', '2237-0.0', '23102-0.0', '806-0.0', '796-0.0', '2040-0.0', \n",
    " '1110-0.0', '816-0.0', '4526-0.0', '826-0.0', '1747-0.0', '100580-0.0', \n",
    " '100240-0.0', '2267-0.0', '757-0.0', '24020-0.0', '1498-0.0', '100390-0.0',\n",
    " '21002-0.0', '79-0.0', '1120-0.0', '24014-0.0', '1060-0.0', '50-0.0', '1050-0.0', \n",
    " '34-0.0', '104670-0.0', '24021-0.0', '1757-0.0', '23105-0.0', '1727-0.0', '24009-0.0',\n",
    " '1140-0.0', '189-0.0', '1150-0.0'] ; len(not_found_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcbb2083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary named var_temp. The keys are original variable names and the values are the new names that we want to assign to them.\n",
    "# The general structure is 'original_name' : 'new_name'. This dictionary is used for renaming variables from the original dataset,\n",
    "# making the variable names more understandable and easier to work with.\n",
    "# This dictionary will contain general external exposures.\n",
    "var_temp_1 = {'eid': 'f.eid',\n",
    "    '31-0.0' : 'Sex',\n",
    "# '34-0.0': 'Age',\n",
    " '48-0.0': 'Waist_circumference',\n",
    " '49-0.0': 'Hip_circumference',\n",
    "# '50-0.0': 'Standing_height',\n",
    " '54-0.0': 'Assessment_centre',\n",
    "# '79-0.0': 'Alcohol_day',\n",
    "# '189-0.0': 'Townsendeprivation',\n",
    " '670-0.0': 'Home_type',\n",
    " '680-0.0': 'Home_status',\n",
    "# '757-0.0': 'Time_employed',\n",
    "# '767-0.0': 'Length_working_week',\n",
    "# '796-0.0': 'Distance_home_job',\n",
    "# '806-0.0': 'Job_walking/standing',\n",
    "# '816-0.0': 'Job_heavy/physical',\n",
    "# '826-0.0': 'Job_shift_work',\n",
    " '845-0.0': 'Age_full_education',\n",
    "# '1050-0.0': 'Time_outdoors_summer',\n",
    "# '1060-0.0': 'Time_outdoors_winter',\n",
    "# '1110-0.0': 'Length_phone_use',\n",
    "# '1120-0.0': 'Weekly_phone_3_months',\n",
    "# '1140-0.0': 'Difference_phone_2_years',\n",
    "# '1150-0.0': 'Side_head_phone',\n",
    " '1160-0.0': 'Sleep_duration',            \n",
    " '1190-0.0': 'Nap_during_day',\n",
    " '1200-0.0': 'Sleeplessness',\n",
    " '1220-0.0': 'Daytime_dozing/sleeping',\n",
    " '1239-0.0': 'Current_smoking',\n",
    " '1249-0.0': 'Past_smoking',\n",
    " '1289-0.0': 'Cooked_vegetable_intake',\n",
    " '1299-0.0': 'Salad/raw_vegetable_intake',\n",
    " '1309-0.0': 'Fresh_fruit_intake',\n",
    " '1319-0.0': 'Dried_fruit_intake',\n",
    " '1329-0.0': 'Oily_fish_intake',\n",
    " '1339-0.0': 'Non_oily_fish_intake',\n",
    " '1349-0.0': 'Processed_meat_intake',\n",
    " '1359-0.0': 'Poultry_intake',\n",
    " '1369-0.0': 'Beef_intake',\n",
    " '1379-0.0': 'Lamb/mutton_intake',\n",
    " '1389-0.0': 'Pork_intake',\n",
    " '1408-0.0': 'Cheese_intake',\n",
    " '1418-0.0': 'Milk_type',\n",
    " '1428-0.0': 'Spread_type',\n",
    " '1438-0.0': 'Bread_intake',\n",
    " '1448-0.0': 'Bread_type',\n",
    " '1458-0.0': 'Cereal_intake',\n",
    " '1468-0.0': 'Cereal_type',\n",
    " '1478-0.0': 'Salt_added',\n",
    " '1488-0.0': 'Tea_intake',\n",
    "# '1498-0.0': 'Coffee_intake',\n",
    " '1508-0.0': 'Coffee_type',\n",
    " '1528-0.0': 'Water_intake',\n",
    " '1538-0.0': 'Dietary_changes_5years',\n",
    " '1548-0.0': 'Variation_diet',\n",
    "# '1727-0.0': 'Skin_tanning',            \n",
    "# '1747-0.0': 'Hair_colour',\n",
    "# '1757-0.0': 'Facial_ageing',\n",
    "# '2040-0.0': 'Risk_taking',\n",
    " '2050-0.0': 'Depressed_2weeks',\n",
    " '2060-0.0': 'Unenthusiasm_2weeks',\n",
    " '2070-0.0': 'Tenseness_2weeks',\n",
    " '2080-0.0': 'Tiredness_2weeks',\n",
    " '2090-0.0': 'Seen_doctor',\n",
    " '2100-0.0': 'Seen_sychiatrist',\n",
    "# '2237-0.0': 'Plays_computer_games',            \n",
    "# '2267-0.0': 'Sun/uv_protection',\n",
    "# '4526-0.0': 'Happiness',\n",
    " '4598-0.0': 'Ever_depressed_1week',\n",
    " '4609-0.0': 'Longest_depression',            \n",
    " '4620-0.0': 'Number_depression',\n",
    " '4631-0.0': 'Ever_unenthusiastic_1week',\n",
    " '6138-0.0': 'Qualifications_0', ###\n",
    " '6138-0.1': 'Qualifications_1', ###\n",
    " '6138-0.2': 'Qualifications_2', ###\n",
    " '6138-0.3': 'Qualifications_3', ###\n",
    " '6138-0.4': 'Qualifications_4', ###\n",
    " '6138-0.5': 'Qualifications_5', ###\n",
    " '6142-0.0': 'Employment_status_0', ###\n",
    " '6142-0.1': 'Employment_status_1', ###\n",
    " '6142-0.2': 'Employment_status_2', ###\n",
    " '6142-0.3': 'Employment_status_3', ###\n",
    " '6142-0.4': 'Employment_status_4', ###\n",
    " '6142-0.5': 'Employment_status_5', ###\n",
    " '6142-0.6': 'Employment_status_6', ###\n",
    " '20117-0.0': 'Drinker_status',           \n",
    " '20126-0.0': 'Bipolar_status',\n",
    "# '20127-0.0': 'Neuroticism_score',\n",
    " '20405-0.0': 'Recommend_reduction_alcohol',\n",
    " '20411-0.0': 'Injury_drinking',\n",
    " '20414-0.0': 'Frequency_drinking',            \n",
    " '20453-0.0': 'Ever_cannabis',\n",
    " '20456-0.0': 'Ever_illicit_drug',\n",
    " '20495-0.0': 'Avoided_activities_1month',\n",
    " '20497-0.0': 'Disturbing_thoughts_1month',\n",
    " '20498-0.0': 'Upset_reminded_1month',            \n",
    " '20521-0.0': 'Belittlement_partner',\n",
    " '20522-0.0': 'Confiding_relationship',\n",
    " '20523-0.0': 'Physical_violence_partner',\n",
    " '20524-0.0': 'Sexual_interference_without_consent',\n",
    " '20525-0.0': 'Able_to_pay_rent/mortgage',            \n",
    " '20526-0.0': 'Accident_life-threatening',\n",
    " '20529-0.0': 'Victim_crime',\n",
    " '20530-0.0': 'Witnessed_death',\n",
    " '20531-0.0': 'Victim_sexual_assault',\n",
    " '21000-0.0': 'Ethnic',            \n",
    " '21001-0.0': 'BMI',\n",
    "# '21002-0.0': 'Weight',\n",
    " '21022-0.0': 'Age',\n",
    " '23099-0.0': 'Body_fat_percentage',\n",
    " '23101-0.0': 'Body_fat-free_mass',\n",
    "# '23102-0.0': 'Body_water_mass',            \n",
    "# '23105-0.0': 'Basal_metabolic_rate',\n",
    "# '24009-0.0': 'Traffic_intensity',\n",
    "# '24014-0.0': 'Close_major_road',\n",
    "# '24020-0.0': 'Daytime_noise_pollution',\n",
    "# '24021-0.0': 'Evening_noise_pollution',            \n",
    "# '100240-0.0': 'Coffee_consumed',\n",
    "# '100390-0.0': 'Tea_consumed',\n",
    "# '100580-0.0': 'Alcohol_consumed',\n",
    "# '104670-0.0': 'Vitamin_supplement_user'\n",
    "             }        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "100f858e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = list(var_temp_1.keys())\n",
    "cut_keys = [key.split('-')[0] for key in keys]\n",
    "unique_cut_keys = list(set(cut_keys))\n",
    "len(unique_cut_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cd9171a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variables that we had in a previous version that are no longer available\n",
    "not_found_2 = ['1737-0.0', '1697-0.0', '1777-0.0', '1687-0.0', '1707-0.0']; len(not_found_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f17ee4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary named var_temp_2. The keys are original variable names and the values are the new names that we want to assign to them.\n",
    "# The general structure is 'original_name' : 'new_name'. This dictionary is used for renaming variables from the original dataset,\n",
    "# making the variable names more understandable and easier to work with.\n",
    "# This dictionary will contain Early Cause Factors.\n",
    "var_temp_2 ={'eid': 'f.eid',\n",
    "            '1677-0.0':'Breastfed_baby',\n",
    "#            '1687-0.0':'Size_age10',\n",
    "#            '1697-0.0':'Height_age10',\n",
    "#            '1707-0.0':'Handedness',\n",
    "#            '1737-0.0':'Childhood_sunburn_occasions',\n",
    "            '1767-0.0':'Adopted_child',\n",
    "#            '1777-0.0':'Part_multiple',\n",
    "            '1787-0.0':'Maternal_smoking_around_birth',\n",
    "            '20487-0.0':'Hated_family_member_child',\n",
    "            '20488-0.0':'Abused_family_child',\n",
    "            '20489-0.0':'Felt_loved_child',\n",
    "            '20490-0.0':'Sexually_molested_child',\n",
    "            '20491-0.0':'Someone_take_doctor_child'}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "839c0adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = list(var_temp_2.keys())\n",
    "cut_keys = [key.split('-')[0] for key in keys]\n",
    "unique_cut_keys = list(set(cut_keys))\n",
    "len(unique_cut_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54613770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables that we had in a previous version that are no longer available\n",
    "not_found_3 = ['30600-0.0', '30080-0.0', '30020-0.0', '30890-0.0', '30720-0.0', '30650-0.0', '30830-0.0', \n",
    "               '30670-0.0', '30610-0.0', '30800-0.0', '30880-0.0', '30000-0.0', '30150-0.0', '30620-0.0', \n",
    "               '30680-0.0', '30140-0.0', '30810-0.0', '30820-0.0', '30730-0.0', '30700-0.0']; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d42eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary named var_temp_3. The keys are original variable names and the values are the new names that we want to assign to them.\n",
    "# The general structure is 'original_name' : 'new_name'. This dictionary is used for renaming variables from the original dataset,\n",
    "# making the variable names more understandable and easier to work with.\n",
    "# This dictionary will contain internal exposures (blood).\n",
    "var_temp_3 ={'eid': 'f.eid',\n",
    "#            '30600-0.0':'Albumin',\n",
    "#            '30610-0.0':'Alkanine_phosphatase',\n",
    "#            '30620-0.0':'Alanine_aminotransferase',\n",
    "            '30630-0.0':'APOA',\n",
    "            '30640-0.0':'APOB',\n",
    "#            '30650-0.0':'Aspartate_aminotransferase',\n",
    "#            '30680-0.0':'Calcium',\n",
    "            '30690-0.0':'Cholesterol',\n",
    "#            '30700-0.0':'Creatinine',\n",
    "            '30710-0.0':'CRP',\n",
    "#            '30730-0.0':'GGT',\n",
    "            '30740-0.0':'Glucose',\n",
    "            '30760-0.0':'HDL',\n",
    "#            '30780-0.0':'LDL',\n",
    "#            '30790-0.0':'Lipoprotein_A',\n",
    "            '30870-0.0':'Triglyceride',\n",
    "#            '30810-0.0':'Phosphate',\n",
    "#            '30820-0.0':'Rheumatoid_factor',\n",
    "#            '30880-0.0':'Uric_acid',\n",
    "#            '30670-0.0':'Urea',\n",
    "#            '30720-0.0':'Cystatin_C',\n",
    "            '30770-0.0':'IGF_1',\n",
    "#            '30890-0.0':'Vitamin_D',\n",
    "#            '30800-0.0':'Oestradiol',\n",
    "            '30850-0.0':'Testosterone',\n",
    "#            '30830-0.0':'SHBG',\n",
    "            '30750-0.0':'HbA1c',\n",
    "#            '30020-0.0':'Haemoglobin_concentration',\n",
    "#            '30080-0.0':'Platelets_count',\n",
    "#            '30000-0.0':'White_blood_cell_count',\n",
    "#            '30150-0.0':'Eosinophil_count',\n",
    "#            '30140-0.0':'Neutrophil_count'\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2c314a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = list(var_temp_3.keys())\n",
    "cut_keys = [key.split('-')[0] for key in keys]\n",
    "unique_cut_keys = list(set(cut_keys))\n",
    "len(unique_cut_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2a05d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7858f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('/Users/marinacamacho/Desktop/Master_I/Raw_Data/ukb46359.csv', usecols = var_temp_1.keys())\n",
    "df_1 = df_1.rename(columns = var_temp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66dca745",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv('/Users/marinacamacho/Desktop/Master_I/Raw_Data/ukb46359.csv', usecols = var_temp_2.keys())\n",
    "df_2 = df_2.rename(columns = var_temp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c72e84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = pd.read_csv('/Users/marinacamacho/Desktop/Master_I/Raw_Data/ukb46359.csv', usecols = var_temp_3.keys())\n",
    "df_3 = df_3.rename(columns = var_temp_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f3ce17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to replace certain values\n",
    "variables = ['Age_full_education','Sleep_duration','Nap_during_day',\n",
    "             'Sleeplessness','Daytime_dozing/sleeping','Drinker_status',\n",
    "             'Frequency_drinking','Ever_cannabis','Ever_illicit_drug','Injury_drinking',\n",
    "             'Recommend_reduction_alcohol','Current_smoking','Past_smoking',\n",
    "             'Cooked_vegetable_intake','Salad/raw_vegetable_intake',\n",
    "             'Fresh_fruit_intake','Dried_fruit_intake','Oily_fish_intake',\n",
    "             'Non_oily_fish_intake','Processed_meat_intake','Poultry_intake',\n",
    "             'Beef_intake','Lamb/mutton_intake','Pork_intake','Cheese_intake',\n",
    "             #'Home_status',\n",
    "             #'Home_type',\n",
    "             #'Milk_type','Spread_type',\n",
    "             'Bread_intake',\n",
    "             #'Bread_type',\n",
    "             'Cereal_intake',\n",
    "             #'Cereal_type',\n",
    "             'Salt_added','Tea_intake',\n",
    "             #'Coffee_type',\n",
    "             'Water_intake','Dietary_changes_5years',\n",
    "             'Variation_diet','Belittlement_partner','Confiding_relationship',\n",
    "             'Physical_violence_partner','Sexual_interference_without_consent',\n",
    "             'Able_to_pay_rent/mortgage','Accident_life-threatening',\n",
    "             'Victim_crime','Witnessed_death','Victim_sexual_assault',\n",
    "             'Avoided_activities_1month','Disturbing_thoughts_1month',\n",
    "             'Upset_reminded_1month','Ever_depressed_1week',\n",
    "             'Longest_depression','Number_depression',\n",
    "             #'Bipolar_status',\n",
    "             #'Neuroticism_score',\n",
    "             'Ever_unenthusiastic_1week',\n",
    "             'Depressed_2weeks','Unenthusiasm_2weeks','Tenseness_2weeks',\n",
    "             'Tiredness_2weeks','Seen_doctor','Seen_sychiatrist',\n",
    "             'Ethnic',\n",
    "             'Age']\n",
    "\n",
    "# Loop over each column in the list of variables\n",
    "for col in variables:\n",
    "    # In each column, replace the values -1,-2,-3,-10,-121,-818 with np.NaN,100,np.NaN,0.5,np.NaN,np.NaN respectively.\n",
    "    # Notice that this values were coding for specific meanings in this variables, see: https://biobank.ndph.ox.ac.uk/ukb/search.cgi.\n",
    "    df_1[col] = df_1[col].replace([-1,-2,-3,-10,-121,-818],[np.NaN,100,np.NaN,0.5,np.NaN,np.NaN])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b115510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['Current_smoking']=df_1['Current_smoking'].replace([2,1], [1,2])\n",
    "df_1['Past_smoking']=df_1['Past_smoking'].replace([0,1,3,4], [4,3,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7194f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['Breastfed_baby','Adopted_child','Maternal_smoking_around_birth',\n",
    "             'Hated_family_member_child','Abused_family_child','Felt_loved_child',\n",
    "             'Sexually_molested_child','Someone_take_doctor_child']\n",
    "\n",
    "# Loop over each column in the list of variables and replace special values for appropiate ones\n",
    "for col in variables:\n",
    "    df_2[col] = df_2[col].replace([-1,-2,-3,-10,-121,-818],[np.NaN,999,np.NaN,0.5,np.NaN,np.NaN])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee23521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f2f22cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map the code to its corresponding meaning\n",
    "code_meaning = {1.0: 'White',\n",
    "       2.0: 'Brown',      \n",
    "       3.0: 'WholeMealgrain',\n",
    "       4.0: 'OtherBread',\n",
    "        -1: 'Do_not_know',\n",
    "        -3: 'Prefer_not_to_answer'}\n",
    "\n",
    "# Specify the column names you want to extract\n",
    "columns_to_extract = ['Bread_type']\n",
    "\n",
    "# Create a new dataframe with only the extracted columns\n",
    "extracted_df = df_1[columns_to_extract].copy()\n",
    "\n",
    "# Create a new dataframe to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each code and meaning in the dictionary\n",
    "for code, meaning in code_meaning.items():\n",
    "    # Check if the code is positive or negative\n",
    "    is_positive = code > 0\n",
    "\n",
    "    # Create a boolean mask indicating where the code is present in the extracted columns\n",
    "    code_mask = extracted_df.isin([code])\n",
    "\n",
    "    # Count the occurrences of the code in each row\n",
    "    code_counts = code_mask.sum(axis=1)\n",
    "\n",
    "    # Create a new column with the meaning and initialize it as 1 if the code is present, else 0\n",
    "    result_df[meaning] = np.where(code_counts > 0, 1, 0)\n",
    "\n",
    "# Concatenate the result dataframe with the original dataframe\n",
    "df_1 = pd.concat([df_1, result_df], axis=1)\n",
    "\n",
    "df_1['White'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['White'])\n",
    "df_1['Brown'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Brown'])\n",
    "df_1['WholeMealgrain'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['WholeMealgrain'])\n",
    "df_1['OtherBread'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['OtherBread'])\n",
    "\n",
    "df_1['White'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['White'])\n",
    "df_1['Brown'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Brown'])\n",
    "df_1['WholeMealgrain'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['WholeMealgrain'])\n",
    "df_1['OtherBread'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['OtherBread'])\n",
    "\n",
    "del df_1['Do_not_know']\n",
    "del df_1['Prefer_not_to_answer']\n",
    "del df_1['Bread_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abf13f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map the code to its corresponding meaning\n",
    "code_meaning = {1: 'Full_cream',\n",
    "    2: 'Semi-skimmed',\n",
    "    3: 'Skimmed',\n",
    "    4: 'Soya',\n",
    "    5: 'Other_type_of_milk',\n",
    "    6: 'Never/rarely_have_milk',\n",
    "    -1: 'Do_not_know',\n",
    "    -3: 'Prefer_not_to_answer'}\n",
    "\n",
    "# Specify the column names you want to extract\n",
    "columns_to_extract = ['Milk_type']\n",
    "\n",
    "# Create a new dataframe with only the extracted columns\n",
    "extracted_df = df_1[columns_to_extract].copy()\n",
    "\n",
    "# Create a new dataframe to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each code and meaning in the dictionary\n",
    "for code, meaning in code_meaning.items():\n",
    "    # Check if the code is positive or negative\n",
    "    is_positive = code > 0\n",
    "\n",
    "    # Create a boolean mask indicating where the code is present in the extracted columns\n",
    "    code_mask = extracted_df.isin([code])\n",
    "\n",
    "    # Count the occurrences of the code in each row\n",
    "    code_counts = code_mask.sum(axis=1)\n",
    "\n",
    "    # Create a new column with the meaning and initialize it as 1 if the code is present, else 0\n",
    "    result_df[meaning] = np.where(code_counts > 0, 1, 0)\n",
    "\n",
    "# Concatenate the result dataframe with the original dataframe\n",
    "df_1 = pd.concat([df_1, result_df], axis=1)\n",
    "\n",
    "df_1['Full_cream'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Full_cream'])\n",
    "df_1['Semi-skimmed'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Semi-skimmed'])\n",
    "df_1['Skimmed'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Skimmed'])\n",
    "df_1['Soya'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Soya'])\n",
    "df_1['Other_type_of_milk'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Other_type_of_milk'])\n",
    "df_1['Never/rarely_have_milk'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Never/rarely_have_milk'])\n",
    "\n",
    "df_1['Full_cream'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Full_cream'])\n",
    "df_1['Semi-skimmed'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Semi-skimmed'])\n",
    "df_1['Skimmed'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Skimmed'])\n",
    "df_1['Soya'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Soya'])\n",
    "df_1['Other_type_of_milk'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Other_type_of_milk'])\n",
    "df_1['Never/rarely_have_milk'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Never/rarely_have_milk'])\n",
    "\n",
    "del df_1['Do_not_know']\n",
    "del df_1['Prefer_not_to_answer']\n",
    "del df_1['Milk_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3433ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map the code to its corresponding meaning\n",
    "code_meaning = {1: 'Butter/spreadable_butter',\n",
    "    2: 'Flora_Pro-Active/Benecol',\n",
    "    3: 'Other_type_of_spread/margarine',\n",
    "    0: 'Never/rarely_use_spread',\n",
    "    -1: 'Do_not_know',\n",
    "    -3: 'Prefer_not_to_answer'}\n",
    "\n",
    "\n",
    "# Specify the column names you want to extract\n",
    "columns_to_extract = ['Spread_type']\n",
    "\n",
    "# Create a new dataframe with only the extracted columns\n",
    "extracted_df = df_1[columns_to_extract].copy()\n",
    "\n",
    "# Create a new dataframe to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each code and meaning in the dictionary\n",
    "for code, meaning in code_meaning.items():\n",
    "    # Check if the code is positive or negative\n",
    "    is_positive = code > 0\n",
    "\n",
    "    # Create a boolean mask indicating where the code is present in the extracted columns\n",
    "    code_mask = extracted_df.isin([code])\n",
    "\n",
    "    # Count the occurrences of the code in each row\n",
    "    code_counts = code_mask.sum(axis=1)\n",
    "\n",
    "    # Create a new column with the meaning and initialize it as 1 if the code is present, else 0\n",
    "    result_df[meaning] = np.where(code_counts > 0, 1, 0)\n",
    "\n",
    "# Concatenate the result dataframe with the original dataframe\n",
    "df_1 = pd.concat([df_1, result_df], axis=1)\n",
    "\n",
    "df_1['Butter/spreadable_butter'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Butter/spreadable_butter'])\n",
    "df_1['Flora_Pro-Active/Benecol'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Flora_Pro-Active/Benecol'])\n",
    "df_1['Other_type_of_spread/margarine'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Other_type_of_spread/margarine'])\n",
    "df_1['Never/rarely_use_spread'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Never/rarely_use_spread'])\n",
    "\n",
    "df_1['Butter/spreadable_butter'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Butter/spreadable_butter'])\n",
    "df_1['Flora_Pro-Active/Benecol'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Flora_Pro-Active/Benecol'])\n",
    "df_1['Other_type_of_spread/margarine'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Other_type_of_spread/margarine'])\n",
    "df_1['Never/rarely_use_spread'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Never/rarely_use_spread'])\n",
    "\n",
    "del df_1['Do_not_know']\n",
    "del df_1['Prefer_not_to_answer']\n",
    "del df_1['Spread_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76d9c826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map the code to its corresponding meaning\n",
    "code_meaning = {1: 'Bran_cereal',\n",
    "    2: 'Biscuit_cereal',\n",
    "    3: 'Oat_cereal',\n",
    "    4: 'Muesli',\n",
    "    5: 'Other_cereal',\n",
    "    -1: 'Do_not_know',\n",
    "    -3: 'Prefer_not_to_answer'}\n",
    "\n",
    "# Specify the column names you want to extract\n",
    "columns_to_extract = ['Cereal_type']\n",
    "\n",
    "# Create a new dataframe with only the extracted columns\n",
    "extracted_df = df_1[columns_to_extract].copy()\n",
    "\n",
    "# Create a new dataframe to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each code and meaning in the dictionary\n",
    "for code, meaning in code_meaning.items():\n",
    "    # Check if the code is positive or negative\n",
    "    is_positive = code > 0\n",
    "\n",
    "    # Create a boolean mask indicating where the code is present in the extracted columns\n",
    "    code_mask = extracted_df.isin([code])\n",
    "\n",
    "    # Count the occurrences of the code in each row\n",
    "    code_counts = code_mask.sum(axis=1)\n",
    "\n",
    "    # Create a new column with the meaning and initialize it as 1 if the code is present, else 0\n",
    "    result_df[meaning] = np.where(code_counts > 0, 1, 0)\n",
    "\n",
    "# Concatenate the result dataframe with the original dataframe\n",
    "df_1 = pd.concat([df_1, result_df], axis=1)\n",
    "\n",
    "df_1['Bran_cereal'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Bran_cereal'])\n",
    "df_1['Biscuit_cereal'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Biscuit_cereal'])\n",
    "df_1['Oat_cereal'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Oat_cereal'])\n",
    "df_1['Muesli'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Muesli'])\n",
    "df_1['Other_cereal'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Other_cereal'])\n",
    "\n",
    "df_1['Bran_cereal'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Bran_cereal'])\n",
    "df_1['Biscuit_cereal'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Biscuit_cereal'])\n",
    "df_1['Oat_cereal'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Oat_cereal'])\n",
    "df_1['Muesli'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Muesli'])\n",
    "df_1['Other_cereal'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Other_cereal'])\n",
    "\n",
    "del df_1['Do_not_know']\n",
    "del df_1['Prefer_not_to_answer']\n",
    "del df_1['Cereal_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "815fdc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map the code to its corresponding meaning\n",
    "code_meaning = {1: 'Decaffeinated_coffee',\n",
    "    2: 'Instant_coffee',\n",
    "    3: 'Ground_coffee',\n",
    "    4: 'Other_coffee',\n",
    "    -1: 'Do_not_know',\n",
    "    -3: 'Prefer_not_to_answer'}\n",
    "\n",
    "# Specify the column names you want to extract\n",
    "columns_to_extract = ['Coffee_type']\n",
    "\n",
    "# Create a new dataframe with only the extracted columns\n",
    "extracted_df = df_1[columns_to_extract].copy()\n",
    "\n",
    "# Create a new dataframe to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each code and meaning in the dictionary\n",
    "for code, meaning in code_meaning.items():\n",
    "    # Check if the code is positive or negative\n",
    "    is_positive = code > 0\n",
    "\n",
    "    # Create a boolean mask indicating where the code is present in the extracted columns\n",
    "    code_mask = extracted_df.isin([code])\n",
    "\n",
    "    # Count the occurrences of the code in each row\n",
    "    code_counts = code_mask.sum(axis=1)\n",
    "\n",
    "    # Create a new column with the meaning and initialize it as 1 if the code is present, else 0\n",
    "    result_df[meaning] = np.where(code_counts > 0, 1, 0)\n",
    "\n",
    "# Concatenate the result dataframe with the original dataframe\n",
    "df_1 = pd.concat([df_1, result_df], axis=1)\n",
    "\n",
    "df_1['Decaffeinated_coffee'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Decaffeinated_coffee'])\n",
    "df_1['Instant_coffee'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Instant_coffee'])\n",
    "df_1['Ground_coffee'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Ground_coffee'])\n",
    "df_1['Other_coffee'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Other_coffee'])\n",
    "\n",
    "df_1['Decaffeinated_coffee'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Decaffeinated_coffee'])\n",
    "df_1['Instant_coffee'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Instant_coffee'])\n",
    "df_1['Ground_coffee'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Ground_coffee'])\n",
    "df_1['Other_coffee'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Other_coffee'])\n",
    "\n",
    "del df_1['Do_not_know']\n",
    "del df_1['Prefer_not_to_answer']\n",
    "del df_1['Coffee_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b414b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map the code to its corresponding meaning\n",
    "code_meaning = {1: 'House/bungalow',\n",
    "    2: 'Flat/maisonette/apartment',\n",
    "    3: 'Mobile/temporary_structure',\n",
    "    4: 'Sheltered_accommodation',\n",
    "    5: 'Care_home',\n",
    "    -1: 'Do_not_know',\n",
    "    -3: 'Prefer_not_to_answer'}\n",
    "\n",
    "# Specify the column names you want to extract\n",
    "columns_to_extract = ['Home_type']\n",
    "\n",
    "# Create a new dataframe with only the extracted columns\n",
    "extracted_df = df_1[columns_to_extract].copy()\n",
    "\n",
    "# Create a new dataframe to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each code and meaning in the dictionary\n",
    "for code, meaning in code_meaning.items():\n",
    "    # Check if the code is positive or negative\n",
    "    is_positive = code > 0\n",
    "\n",
    "    # Create a boolean mask indicating where the code is present in the extracted columns\n",
    "    code_mask = extracted_df.isin([code])\n",
    "\n",
    "    # Count the occurrences of the code in each row\n",
    "    code_counts = code_mask.sum(axis=1)\n",
    "\n",
    "    # Create a new column with the meaning and initialize it as 1 if the code is present, else 0\n",
    "    result_df[meaning] = np.where(code_counts > 0, 1, 0)\n",
    "\n",
    "# Concatenate the result dataframe with the original dataframe\n",
    "df_1 = pd.concat([df_1, result_df], axis=1)\n",
    "\n",
    "df_1['House/bungalow'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['House/bungalow'])\n",
    "df_1['Flat/maisonette/apartment'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Flat/maisonette/apartment'])\n",
    "df_1['Mobile/temporary_structure'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Mobile/temporary_structure'])\n",
    "df_1['Sheltered_accommodation'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Sheltered_accommodation'])\n",
    "df_1['Care_home'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Care_home'])\n",
    "\n",
    "df_1['House/bungalow'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['House/bungalow'])\n",
    "df_1['Flat/maisonette/apartment'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Flat/maisonette/apartment'])\n",
    "df_1['Sheltered_accommodation/temporary_structure'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Mobile/temporary_structure'])\n",
    "df_1['Muesli'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Sheltered_accommodation'])\n",
    "df_1['Care_home'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Care_home'])\n",
    "\n",
    "del df_1['Do_not_know']\n",
    "del df_1['Prefer_not_to_answer']\n",
    "del df_1['Home_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2a88b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map the code to its corresponding meaning\n",
    "code_meaning = {0: 'No_Bipolar_Depression',\n",
    "    1: 'Bipolar_I_Disorder',\n",
    "    2: 'Bipolar_II_Disorder',\n",
    "    3: 'Recurrent_severe_depression',\n",
    "    4: 'Recurrent_moderate_depression',\n",
    "    5: 'Single_depression_episode'}\n",
    "\n",
    "# Specify the column names you want to extract\n",
    "columns_to_extract = ['Bipolar_status']\n",
    "\n",
    "# Create a new dataframe with only the extracted columns\n",
    "extracted_df = df_1[columns_to_extract].copy()\n",
    "\n",
    "# Create a new dataframe to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each code and meaning in the dictionary\n",
    "for code, meaning in code_meaning.items():\n",
    "    # Check if the code is positive or negative\n",
    "    is_positive = code > 0\n",
    "\n",
    "    # Create a boolean mask indicating where the code is present in the extracted columns\n",
    "    code_mask = extracted_df.isin([code])\n",
    "\n",
    "    # Count the occurrences of the code in each row\n",
    "    code_counts = code_mask.sum(axis=1)\n",
    "\n",
    "    # Create a new column with the meaning and initialize it as 1 if the code is present, else 0\n",
    "    result_df[meaning] = np.where(code_counts > 0, 1, 0)\n",
    "\n",
    "# Concatenate the result dataframe with the original dataframe\n",
    "df_1 = pd.concat([df_1, result_df], axis=1)\n",
    "\n",
    "del df_1['No_Bipolar_Depression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0fe6ea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'6138-0.0': 'Qualifications'\n",
    "#'6142-0.0': 'Employment_status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43729c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map the code to its corresponding meaning\n",
    "code_meaning = {\n",
    "    -7.0: 'None_of_the_above',\n",
    "    -3.0: 'Prefer_not_to_answer',\n",
    "    1.0: 'University',  \n",
    "    2.0: 'A/AS',\n",
    "    3.0: 'O/GCSE',\n",
    "    4.0: 'CSE',\n",
    "    5.0: 'NVQ/HND/HNC',\n",
    "    6.0: 'Professional'\n",
    "}\n",
    "\n",
    "# Specify the column names you want to extract\n",
    "columns_to_extract = ['Qualifications_0','Qualifications_1','Qualifications_2',\n",
    "                      'Qualifications_3','Qualifications_4','Qualifications_5']\n",
    "\n",
    "# Create a new dataframe with only the extracted columns\n",
    "extracted_df = df_1[columns_to_extract].copy()\n",
    "\n",
    "# Create a new dataframe to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each code and meaning in the dictionary\n",
    "for code, meaning in code_meaning.items():\n",
    "    \n",
    "    # Create a boolean mask indicating where the code is present in the extracted columns\n",
    "    code_mask = extracted_df.isin([code])\n",
    "\n",
    "    # Count the occurrences of the code in each row\n",
    "    code_counts = code_mask.sum(axis=1)\n",
    "\n",
    "    # Create a new column with the meaning and initialize it as 1 if the code is present, else 0\n",
    "    result_df[meaning] = np.where(code_counts > 0, 1, 0)\n",
    "\n",
    "# Concatenate the result dataframe with the original dataframe\n",
    "df_1 = pd.concat([df_1, result_df], axis=1)\n",
    "\n",
    "df_1['University'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['University'])\n",
    "df_1['A/AS'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['A/AS'])\n",
    "df_1['O/GCSE'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['O/GCSE'])\n",
    "df_1['CSE'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['CSE'])\n",
    "df_1['NVQ/HND/HNC'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['NVQ/HND/HNC'])\n",
    "df_1['Professional'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['Professional'])\n",
    "\n",
    "del df_1['Qualifications_0']\n",
    "del df_1['Qualifications_1']\n",
    "del df_1['Qualifications_2']\n",
    "del df_1['Qualifications_3']\n",
    "del df_1['Qualifications_4']\n",
    "del df_1['Qualifications_5']\n",
    "\n",
    "del df_1['None_of_the_above']\n",
    "del df_1['Prefer_not_to_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb62d3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map the code to its corresponding meaning\n",
    "code_meaning = {\n",
    "    -7.0: 'None_of_the_above',\n",
    "    -3.0: 'Prefer_not_to_answer',\n",
    "    1.0: 'Own_outright',\n",
    "    2.0: 'Own_mortgage', \n",
    "    3.0: 'Rent_local',\n",
    "    4.0: 'Rent_private',      \n",
    "    5.0: 'Pay_rent_mortgage',  \n",
    "    6.0: 'Rent_free',      \n",
    "}\n",
    "\n",
    "# Specify the column names you want to extract\n",
    "columns_to_extract = ['Home_status']\n",
    "\n",
    "# Create a new dataframe with only the extracted columns\n",
    "extracted_df = df_1[columns_to_extract].copy()\n",
    "\n",
    "# Create a new dataframe to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each code and meaning in the dictionary\n",
    "for code, meaning in code_meaning.items():\n",
    "\n",
    "    # Create a boolean mask indicating where the code is present in the extracted columns\n",
    "    code_mask = extracted_df.isin([code])\n",
    "\n",
    "    # Count the occurrences of the code in each row\n",
    "    code_counts = code_mask.sum(axis=1)\n",
    "\n",
    "    # Create a new column with the meaning and initialize it as 1 if the code is present, else 0\n",
    "    result_df[meaning] = np.where(code_counts > 0, 1, 0)\n",
    "\n",
    "# Concatenate the result dataframe with the original dataframe\n",
    "df_1 = pd.concat([df_1, result_df], axis=1)\n",
    "\n",
    "df_1['Own_outright'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.NaN, df_1['Own_outright'])\n",
    "df_1['Own_mortgage'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.NaN, df_1['Own_mortgage'])\n",
    "df_1['Rent_local'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.NaN, df_1['Rent_local'])\n",
    "df_1['Rent_private'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.NaN, df_1['Rent_private'])\n",
    "df_1['Pay_rent_mortgage'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.NaN, df_1['Pay_rent_mortgage'])\n",
    "df_1['Rent_free'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.NaN, df_1['Rent_free'])\n",
    "\n",
    "del df_1['None_of_the_above']\n",
    "del df_1['Prefer_not_to_answer']\n",
    "del df_1['Home_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ecbd220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map the code to its corresponding meaning\n",
    "code_meaning = {\n",
    "    -7.0: 'None_of_the_above',\n",
    "    -3.0: 'Prefer_not_to_answer',\n",
    "    1.0: 'Paid_employment',\n",
    "    2.0: 'Retired', \n",
    "    3.0: 'Looking_after_home',\n",
    "    4.0: 'Unable_to_work',      \n",
    "    5.0: 'Unemployed',  \n",
    "    6.0: 'Unpaid_work',      \n",
    "    7.0: 'Student'\n",
    "}\n",
    "\n",
    "# Specify the column names you want to extract\n",
    "columns_to_extract = ['Employment_status_0','Employment_status_1','Employment_status_2',\n",
    "                      'Employment_status_3','Employment_status_4','Employment_status_5',\n",
    "                      'Employment_status_6']\n",
    "\n",
    "# Create a new dataframe with only the extracted columns\n",
    "extracted_df = df_1[columns_to_extract].copy()\n",
    "\n",
    "# Create a new dataframe to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each code and meaning in the dictionary\n",
    "for code, meaning in code_meaning.items():\n",
    "\n",
    "    # Create a boolean mask indicating where the code is present in the extracted columns\n",
    "    code_mask = extracted_df.isin([code])\n",
    "\n",
    "    # Count the occurrences of the code in each row\n",
    "    code_counts = code_mask.sum(axis=1)\n",
    "\n",
    "    # Create a new column with the meaning and initialize it as 1 if the code is present, else 0\n",
    "    result_df[meaning] = np.where(code_counts > 0, 1, 0)\n",
    "\n",
    "# Concatenate the result dataframe with the original dataframe\n",
    "df_1 = pd.concat([df_1, result_df], axis=1)\n",
    "\n",
    "df_1['Paid_employment'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['Paid_employment'])\n",
    "df_1['Retired'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['Retired'])\n",
    "df_1['Looking_after_home'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['Looking_after_home'])\n",
    "df_1['Unable_to_work'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['Unable_to_work'])\n",
    "df_1['Unemployed'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['Unemployed'])\n",
    "df_1['Unpaid_work'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['Unpaid_work'])\n",
    "df_1['Student'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['Student'])\n",
    "  \n",
    "del df_1['Employment_status_0']\n",
    "del df_1['Employment_status_1']\n",
    "del df_1['Employment_status_2']\n",
    "del df_1['Employment_status_3']\n",
    "del df_1['Employment_status_4']\n",
    "del df_1['Employment_status_5']\n",
    "del df_1['Employment_status_6']\n",
    "\n",
    "del df_1['None_of_the_above']\n",
    "del df_1['Prefer_not_to_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10aa3981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(502481, 118)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3456b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(502481, 9)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "716410d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(502481, 11)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "321e028c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_with_negatives_1 = df_1.columns[df_1.lt(0).any()]; columns_with_negatives_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9c23da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_with_negatives_2 = df_2.columns[df_2.lt(0).any()]; columns_with_negatives_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1594242b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_with_negatives_3 = df_3.columns[df_3.lt(0).any()]; columns_with_negatives_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "384f6122",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.to_csv(r'/Users/marinacamacho/Desktop/Master_I/Raw_Data/Time_0/ukb46359_clean_external.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc953d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.to_csv(r'/Users/marinacamacho/Desktop/Master_I/Raw_Data/Time_0/ukb46359_clean_internal.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "466918d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.to_csv(r'/Users/marinacamacho/Desktop/Master_I/Raw_Data/Time_0/ukb46359_clean_earlycause.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "555c2292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f.eid</th>\n",
       "      <th>Breastfed_baby</th>\n",
       "      <th>Adopted_child</th>\n",
       "      <th>Maternal_smoking_around_birth</th>\n",
       "      <th>Hated_family_member_child</th>\n",
       "      <th>Abused_family_child</th>\n",
       "      <th>Felt_loved_child</th>\n",
       "      <th>Sexually_molested_child</th>\n",
       "      <th>Someone_take_doctor_child</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000028</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000034</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502476</th>\n",
       "      <td>6024770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502477</th>\n",
       "      <td>6024784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502478</th>\n",
       "      <td>6024795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502479</th>\n",
       "      <td>6024804</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502480</th>\n",
       "      <td>6024818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>502481 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          f.eid  Breastfed_baby  Adopted_child  Maternal_smoking_around_birth  \\\n",
       "0       1000010             0.0            0.0                            NaN   \n",
       "1       1000028             1.0            0.0                            0.0   \n",
       "2       1000034             1.0            0.0                            0.0   \n",
       "3       1000045             NaN            0.0                            NaN   \n",
       "4       1000052             0.0            0.0                            0.0   \n",
       "...         ...             ...            ...                            ...   \n",
       "502476  6024770             NaN            0.0                            0.0   \n",
       "502477  6024784             1.0            0.0                            0.0   \n",
       "502478  6024795             0.0            0.0                            1.0   \n",
       "502479  6024804             1.0            0.0                            0.0   \n",
       "502480  6024818             NaN            0.0                            1.0   \n",
       "\n",
       "        Hated_family_member_child  Abused_family_child  Felt_loved_child  \\\n",
       "0                             2.0                  2.0               2.0   \n",
       "1                             NaN                  NaN               NaN   \n",
       "2                             NaN                  NaN               NaN   \n",
       "3                             NaN                  NaN               NaN   \n",
       "4                             NaN                  NaN               NaN   \n",
       "...                           ...                  ...               ...   \n",
       "502476                        0.0                  0.0               4.0   \n",
       "502477                        NaN                  NaN               NaN   \n",
       "502478                        NaN                  NaN               NaN   \n",
       "502479                        0.0                  0.0               3.0   \n",
       "502480                        NaN                  NaN               NaN   \n",
       "\n",
       "        Sexually_molested_child  Someone_take_doctor_child  \n",
       "0                           0.0                        4.0  \n",
       "1                           NaN                        NaN  \n",
       "2                           NaN                        NaN  \n",
       "3                           NaN                        NaN  \n",
       "4                           NaN                        NaN  \n",
       "...                         ...                        ...  \n",
       "502476                      0.0                        4.0  \n",
       "502477                      NaN                        NaN  \n",
       "502478                      NaN                        NaN  \n",
       "502479                      0.0                        4.0  \n",
       "502480                      NaN                        NaN  \n",
       "\n",
       "[502481 rows x 9 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b9646f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f.eid</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Waist_circumference</th>\n",
       "      <th>Hip_circumference</th>\n",
       "      <th>Assessment_centre</th>\n",
       "      <th>Age_full_education</th>\n",
       "      <th>Sleep_duration</th>\n",
       "      <th>Nap_during_day</th>\n",
       "      <th>Sleeplessness</th>\n",
       "      <th>Daytime_dozing/sleeping</th>\n",
       "      <th>...</th>\n",
       "      <th>Rent_private</th>\n",
       "      <th>Pay_rent_mortgage</th>\n",
       "      <th>Rent_free</th>\n",
       "      <th>Paid_employment</th>\n",
       "      <th>Retired</th>\n",
       "      <th>Looking_after_home</th>\n",
       "      <th>Unable_to_work</th>\n",
       "      <th>Unemployed</th>\n",
       "      <th>Unpaid_work</th>\n",
       "      <th>Student</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>11018.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000028</td>\n",
       "      <td>1.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>11020.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>11016.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000045</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>11018.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000052</td>\n",
       "      <td>1.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>11010.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502476</th>\n",
       "      <td>6024770</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>11011.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502477</th>\n",
       "      <td>6024784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>11011.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502478</th>\n",
       "      <td>6024795</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>11009.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502479</th>\n",
       "      <td>6024804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>11017.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502480</th>\n",
       "      <td>6024818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>11020.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>502481 rows Ã— 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          f.eid  Sex  Waist_circumference  Hip_circumference  \\\n",
       "0       1000010  0.0                 74.0              102.0   \n",
       "1       1000028  1.0                120.0              113.0   \n",
       "2       1000034  0.0                 66.0               88.0   \n",
       "3       1000045  1.0                110.0              117.0   \n",
       "4       1000052  1.0                 94.0              100.0   \n",
       "...         ...  ...                  ...                ...   \n",
       "502476  6024770  1.0                110.0              113.0   \n",
       "502477  6024784  0.0                102.0              110.0   \n",
       "502478  6024795  1.0                 99.0               98.0   \n",
       "502479  6024804  0.0                 86.0              111.0   \n",
       "502480  6024818  1.0                 83.0               98.0   \n",
       "\n",
       "        Assessment_centre  Age_full_education  Sleep_duration  Nap_during_day  \\\n",
       "0                 11018.0                 NaN             7.0             2.0   \n",
       "1                 11020.0                16.0             9.0             2.0   \n",
       "2                 11016.0                16.0             5.0             1.0   \n",
       "3                 11018.0                18.0             7.0             2.0   \n",
       "4                 11010.0                 NaN             6.0             2.0   \n",
       "...                   ...                 ...             ...             ...   \n",
       "502476            11011.0                16.0             7.0             2.0   \n",
       "502477            11011.0                16.0             6.0             2.0   \n",
       "502478            11009.0               100.0             7.0             2.0   \n",
       "502479            11017.0                 NaN             6.0             1.0   \n",
       "502480            11020.0                16.0            12.0             2.0   \n",
       "\n",
       "        Sleeplessness  Daytime_dozing/sleeping  ...  Rent_private  \\\n",
       "0                 3.0                      0.0  ...           0.0   \n",
       "1                 2.0                      0.0  ...           0.0   \n",
       "2                 3.0                      0.0  ...           0.0   \n",
       "3                 1.0                      0.0  ...           0.0   \n",
       "4                 3.0                      0.0  ...           0.0   \n",
       "...               ...                      ...  ...           ...   \n",
       "502476            2.0                      0.0  ...           0.0   \n",
       "502477            3.0                      0.0  ...           0.0   \n",
       "502478            2.0                      1.0  ...           0.0   \n",
       "502479            3.0                      0.0  ...           0.0   \n",
       "502480            3.0                      0.0  ...           0.0   \n",
       "\n",
       "        Pay_rent_mortgage  Rent_free  Paid_employment  Retired  \\\n",
       "0                     0.0        0.0                1        0   \n",
       "1                     0.0        0.0                1        1   \n",
       "2                     0.0        0.0                0        1   \n",
       "3                     0.0        0.0                0        1   \n",
       "4                     0.0        0.0                0        0   \n",
       "...                   ...        ...              ...      ...   \n",
       "502476                0.0        0.0                1        0   \n",
       "502477                0.0        0.0                0        1   \n",
       "502478                0.0        0.0                0        0   \n",
       "502479                0.0        0.0                1        0   \n",
       "502480                0.0        0.0                1        0   \n",
       "\n",
       "        Looking_after_home  Unable_to_work  Unemployed  Unpaid_work  Student  \n",
       "0                        0               0           0            0        0  \n",
       "1                        0               0           0            0        0  \n",
       "2                        0               0           0            0        0  \n",
       "3                        0               0           0            0        0  \n",
       "4                        0               0           1            0        0  \n",
       "...                    ...             ...         ...          ...      ...  \n",
       "502476                   0               0           0            0        0  \n",
       "502477                   0               0           0            0        0  \n",
       "502478                   0               0           0            0        0  \n",
       "502479                   0               0           0            0        0  \n",
       "502480                   0               0           0            0        0  \n",
       "\n",
       "[502481 rows x 118 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f01a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
