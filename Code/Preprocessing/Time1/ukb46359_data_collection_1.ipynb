{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b257e737",
   "metadata": {},
   "source": [
    "# Data collection and cleaning: ukb46359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d32046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the json module, which allows us to work with JSON files in Python.\n",
    "import json\n",
    "with open('/Users/marinacamacho/Desktop/Master_I/var.json') as f:\n",
    "    var_temp = json.load(f)\n",
    "\n",
    "# Import the numpy module. Numpy is a library in Python that provides support for large, \n",
    "# multi-dimensional arrays and matrices, along with a large collection of high-level \n",
    "# mathematical functions to operate on these arrays.\n",
    "import numpy as np\n",
    "\n",
    "# Import the pandas module, which allows us to work with data structures and data analysis tools.\n",
    "# Given it's a large dataset, the 'nrows=1' argument is used to read only the first row of the CSV file.\n",
    "import pandas as pd\n",
    "df_ = pd.read_csv('/Users/marinacamacho/Desktop/Master_I/Raw_Data/ukb46359.csv', nrows=1)  # Read only the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b838631e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7868c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eid</th>\n",
       "      <th>31-0.0</th>\n",
       "      <th>44-0.0</th>\n",
       "      <th>44-1.0</th>\n",
       "      <th>44-2.0</th>\n",
       "      <th>44-3.0</th>\n",
       "      <th>48-0.0</th>\n",
       "      <th>48-1.0</th>\n",
       "      <th>48-2.0</th>\n",
       "      <th>48-3.0</th>\n",
       "      <th>...</th>\n",
       "      <th>131414-0.0</th>\n",
       "      <th>131415-0.0</th>\n",
       "      <th>131416-0.0</th>\n",
       "      <th>131417-0.0</th>\n",
       "      <th>131418-0.0</th>\n",
       "      <th>131419-0.0</th>\n",
       "      <th>131420-0.0</th>\n",
       "      <th>131421-0.0</th>\n",
       "      <th>131422-0.0</th>\n",
       "      <th>131423-0.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000010</td>\n",
       "      <td>0</td>\n",
       "      <td>4675</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 4009 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       eid  31-0.0  44-0.0  44-1.0  44-2.0  44-3.0  48-0.0  48-1.0  48-2.0  \\\n",
       "0  1000010       0    4675     NaN     NaN     NaN      74     NaN     NaN   \n",
       "\n",
       "   48-3.0  ...  131414-0.0  131415-0.0  131416-0.0  131417-0.0 131418-0.0  \\\n",
       "0     NaN  ...         NaN         NaN         NaN         NaN        NaN   \n",
       "\n",
       "   131419-0.0  131420-0.0  131421-0.0  131422-0.0  131423-0.0  \n",
       "0         NaN         NaN         NaN         NaN         NaN  \n",
       "\n",
       "[1 rows x 4009 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64e329ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_including_minus_0 = df_.filter(regex='.*-1.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6ca06d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>44-1.0</th>\n",
       "      <th>48-1.0</th>\n",
       "      <th>49-1.0</th>\n",
       "      <th>53-1.0</th>\n",
       "      <th>54-1.0</th>\n",
       "      <th>55-1.0</th>\n",
       "      <th>74-1.0</th>\n",
       "      <th>87-1.0</th>\n",
       "      <th>87-1.1</th>\n",
       "      <th>87-1.2</th>\n",
       "      <th>...</th>\n",
       "      <th>30750-1.0</th>\n",
       "      <th>30760-1.0</th>\n",
       "      <th>30770-1.0</th>\n",
       "      <th>30780-1.0</th>\n",
       "      <th>30790-1.0</th>\n",
       "      <th>30850-1.0</th>\n",
       "      <th>30870-1.0</th>\n",
       "      <th>104900-1.0</th>\n",
       "      <th>104910-1.0</th>\n",
       "      <th>104920-1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3060</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 344 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   44-1.0  48-1.0  49-1.0  53-1.0  54-1.0  55-1.0  74-1.0  87-1.0  87-1.1  \\\n",
       "0     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "   87-1.2  ...  30750-1.0  30760-1.0  30770-1.0  30780-1.0  30790-1.0  \\\n",
       "0     NaN  ...        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "   30850-1.0  30870-1.0  104900-1.0  104910-1.0  104920-1.0  \n",
       "0        NaN        NaN           0        3060          13  \n",
       "\n",
       "[1 rows x 344 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_including_minus_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a04d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27b98c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variables that we had in a previous version that are no longer available\n",
    "not_found_1 = ['767-0.0', '2237-0.0', '23102-0.0', '806-0.0', '796-0.0', '2040-0.0', \n",
    " '1110-0.0', '816-0.0', '4526-0.0', '826-0.0', '1747-0.0', '100580-0.0', \n",
    " '100240-0.0', '2267-0.0', '757-0.0', '24020-0.0', '1498-0.0', '100390-0.0',\n",
    " '21002-0.0', '79-0.0', '1120-0.0', '24014-0.0', '1060-0.0', '50-0.0', '1050-0.0', \n",
    " '34-0.0', '104670-0.0', '24021-0.0', '1757-0.0', '23105-0.0', '1727-0.0', '24009-0.0',\n",
    " '1140-0.0', '189-0.0', '1150-0.0'] ; len(not_found_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34981e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables that we don't have a second assesment, hence cannot be traced\n",
    "not_treaceable_1 = ['20411-1.0', '20495-1.0', '20524-1.0', '20529-1.0', '20521-1.0', '20126-1.0', '20526-1.0', '20525-1.0', '20531-1.0', '20453-1.0', '20497-1.0', '31-1.0', '20522-1.0', '20414-1.0', '20530-1.0', '20405-1.0', '20523-1.0', '20498-1.0', '20456-1.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcbb2083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary named var_temp. The keys are original variable names and the values are the new names that we want to assign to them.\n",
    "# The general structure is 'original_name' : 'new_name'. This dictionary is used for renaming variables from the original dataset,\n",
    "# making the variable names more understandable and easier to work with.\n",
    "# This dictionary will contain general external exposures.\n",
    "var_temp_1 = {'eid': 'f.eid',\n",
    "    '31-0.0' : 'Sex',\n",
    "# '34-0.0': 'Age',\n",
    " '48-1.0': 'Waist_circumference',\n",
    " '49-1.0': 'Hip_circumference',\n",
    "# '50-0.0': 'Standing_height',\n",
    " '54-1.0': 'Assessment_centre',\n",
    "# '79-0.0': 'Alcohol_day',\n",
    "# '189-0.0': 'Townsendeprivation',\n",
    " '670-1.0': 'Home_type',\n",
    " '680-1.0': 'Home_status',\n",
    "# '757-0.0': 'Time_employed',\n",
    "# '767-0.0': 'Length_working_week',\n",
    "# '796-0.0': 'Distance_home_job',\n",
    "# '806-0.0': 'Job_walking/standing',\n",
    "# '816-0.0': 'Job_heavy/physical',\n",
    "# '826-0.0': 'Job_shift_work',\n",
    " '845-1.0': 'Age_full_education',\n",
    "# '1050-0.0': 'Time_outdoors_summer',\n",
    "# '1060-0.0': 'Time_outdoors_winter',\n",
    "# '1110-0.0': 'Length_phone_use',\n",
    "# '1120-0.0': 'Weekly_phone_3_months',\n",
    "# '1140-0.0': 'Difference_phone_2_years',\n",
    "# '1150-0.0': 'Side_head_phone',\n",
    " '1160-1.0': 'Sleep_duration',            \n",
    " '1190-1.0': 'Nap_during_day',\n",
    " '1200-1.0': 'Sleeplessness',\n",
    " '1220-1.0': 'Daytime_dozing/sleeping',\n",
    " '1239-1.0': 'Current_smoking',\n",
    " '1249-1.0': 'Past_smoking',\n",
    " '1289-1.0': 'Cooked_vegetable_intake',\n",
    " '1299-1.0': 'Salad/raw_vegetable_intake',\n",
    " '1309-1.0': 'Fresh_fruit_intake',\n",
    " '1319-1.0': 'Dried_fruit_intake',\n",
    " '1329-1.0': 'Oily_fish_intake',\n",
    " '1339-1.0': 'Non_oily_fish_intake',\n",
    " '1349-1.0': 'Processed_meat_intake',\n",
    " '1359-1.0': 'Poultry_intake',\n",
    " '1369-1.0': 'Beef_intake',\n",
    " '1379-1.0': 'Lamb/mutton_intake',\n",
    " '1389-1.0': 'Pork_intake',\n",
    " '1408-1.0': 'Cheese_intake',\n",
    " '1418-1.0': 'Milk_type',\n",
    " '1428-1.0': 'Spread_type',\n",
    " '1438-1.0': 'Bread_intake',\n",
    " '1448-1.0': 'Bread_type',\n",
    " '1458-1.0': 'Cereal_intake',\n",
    " '1468-1.0': 'Cereal_type',\n",
    " '1478-1.0': 'Salt_added',\n",
    " '1488-1.0': 'Tea_intake',\n",
    "# '1498-0.0': 'Coffee_intake',\n",
    " '1508-1.0': 'Coffee_type',\n",
    " '1528-1.0': 'Water_intake',\n",
    " '1538-1.0': 'Dietary_changes_5years',\n",
    " '1548-1.0': 'Variation_diet',\n",
    "# '1727-0.0': 'Skin_tanning',            \n",
    "# '1747-0.0': 'Hair_colour',\n",
    "# '1757-0.0': 'Facial_ageing',\n",
    "# '2040-0.0': 'Risk_taking',\n",
    " '2050-1.0': 'Depressed_2weeks',\n",
    " '2060-1.0': 'Unenthusiasm_2weeks',\n",
    " '2070-1.0': 'Tenseness_2weeks',\n",
    " '2080-1.0': 'Tiredness_2weeks',\n",
    " '2090-1.0': 'Seen_doctor',\n",
    " '2100-1.0': 'Seen_sychiatrist',\n",
    "# '2237-0.0': 'Plays_computer_games',            \n",
    "# '2267-0.0': 'Sun/uv_protection',\n",
    "# '4526-0.0': 'Happiness',\n",
    " '4598-1.0': 'Ever_depressed_1week',\n",
    " '4609-1.0': 'Longest_depression',            \n",
    " '4620-1.0': 'Number_depression',\n",
    " '4631-1.0': 'Ever_unenthusiastic_1week',\n",
    " '6138-1.0': 'Qualifications_0', ###\n",
    " '6138-1.1': 'Qualifications_1', ###\n",
    " '6138-1.2': 'Qualifications_2', ###\n",
    " '6138-1.3': 'Qualifications_3', ###\n",
    " '6138-1.4': 'Qualifications_4', ###\n",
    " '6138-1.5': 'Qualifications_5', ###\n",
    " '6142-1.0': 'Employment_status_0', ###\n",
    " '6142-1.1': 'Employment_status_1', ###\n",
    " '6142-1.2': 'Employment_status_2', ###\n",
    " '6142-1.3': 'Employment_status_3', ###\n",
    " '6142-1.4': 'Employment_status_4', ###\n",
    " '6142-1.5': 'Employment_status_5', ###\n",
    " '6142-1.6': 'Employment_status_6', ###\n",
    " '20117-1.0': 'Drinker_status',           \n",
    " '20126-0.0': 'Bipolar_status',\n",
    "# '20127-0.0': 'Neuroticism_score',\n",
    " '20405-0.0': 'Recommend_reduction_alcohol',\n",
    " '20411-0.0': 'Injury_drinking',\n",
    " '20414-0.0': 'Frequency_drinking',            \n",
    " '20453-0.0': 'Ever_cannabis',\n",
    " '20456-0.0': 'Ever_illicit_drug',\n",
    " '20495-0.0': 'Avoided_activities_1month',\n",
    " '20497-0.0': 'Disturbing_thoughts_1month',\n",
    " '20498-0.0': 'Upset_reminded_1month',            \n",
    " '20521-0.0': 'Belittlement_partner',\n",
    " '20522-0.0': 'Confiding_relationship',\n",
    " '20523-0.0': 'Physical_violence_partner',\n",
    " '20524-0.0': 'Sexual_interference_without_consent',\n",
    " '20525-0.0': 'Able_to_pay_rent/mortgage',            \n",
    " '20526-0.0': 'Accident_life-threatening',\n",
    " '20529-0.0': 'Victim_crime',\n",
    " '20530-0.0': 'Witnessed_death',\n",
    " '20531-0.0': 'Victim_sexual_assault',\n",
    " '21000-1.0': 'Ethnic',            \n",
    " '21001-1.0': 'BMI',\n",
    "# '21002-0.0': 'Weight',\n",
    " '21022-1.0': 'Age',\n",
    " '23099-1.0': 'Body_fat_percentage',\n",
    " '23101-1.0': 'Body_fat-free_mass',\n",
    "# '23102-0.0': 'Body_water_mass',            \n",
    "# '23105-0.0': 'Basal_metabolic_rate',\n",
    "# '24009-0.0': 'Traffic_intensity',\n",
    "# '24014-0.0': 'Close_major_road',\n",
    "# '24020-0.0': 'Daytime_noise_pollution',\n",
    "# '24021-0.0': 'Evening_noise_pollution',            \n",
    "# '100240-0.0': 'Coffee_consumed',\n",
    "# '100390-0.0': 'Tea_consumed',\n",
    "# '100580-0.0': 'Alcohol_consumed',\n",
    "# '104670-0.0': 'Vitamin_supplement_user'\n",
    "             }        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cd9171a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variables that we had in a previous version that are no longer available\n",
    "not_found_2 = ['1737-0.0', '1697-0.0', '1777-0.0', '1687-0.0', '1707-0.0']; len(not_found_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beb36122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables that we don't have a second assesment, hence cannot be traced\n",
    "not_treaceable_2 = ['20487-1.0', '20488-1.0', '20490-1.0', '20491-1.0', '20489-1.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f17ee4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary named var_temp_2. The keys are original variable names and the values are the new names that we want to assign to them.\n",
    "# The general structure is 'original_name' : 'new_name'. This dictionary is used for renaming variables from the original dataset,\n",
    "# making the variable names more understandable and easier to work with.\n",
    "# This dictionary will contain Early Cause Factors.\n",
    "var_temp_2 ={'eid': 'f.eid',\n",
    "            '1677-1.0':'Breastfed_baby',\n",
    "#            '1687-0.0':'Size_age10',\n",
    "#            '1697-0.0':'Height_age10',\n",
    "#            '1707-0.0':'Handedness',\n",
    "#            '1737-0.0':'Childhood_sunburn_occasions',\n",
    "            '1767-1.0':'Adopted_child',\n",
    "#            '1777-0.0':'Part_multiple',\n",
    "            '1787-1.0':'Maternal_smoking_around_birth',\n",
    "            '20487-0.0':'Hated_family_member_child',\n",
    "            '20488-0.0':'Abused_family_child',\n",
    "            '20489-0.0':'Felt_loved_child',\n",
    "            '20490-0.0':'Sexually_molested_child',\n",
    "            '20491-0.0':'Someone_take_doctor_child'}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54613770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables that we had in a previous version that are no longer available\n",
    "not_found_3 = ['30600-0.0', '30080-0.0', '30020-0.0', '30890-0.0', '30720-0.0', '30650-0.0', '30830-0.0', \n",
    "               '30670-0.0', '30610-0.0', '30800-0.0', '30880-0.0', '30000-0.0', '30150-0.0', '30620-0.0', \n",
    "               '30680-0.0', '30140-0.0', '30810-0.0', '30820-0.0', '30730-0.0', '30700-0.0']; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d42eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary named var_temp_3. The keys are original variable names and the values are the new names that we want to assign to them.\n",
    "# The general structure is 'original_name' : 'new_name'. This dictionary is used for renaming variables from the original dataset,\n",
    "# making the variable names more understandable and easier to work with.\n",
    "# This dictionary will contain internal exposures (blood).\n",
    "var_temp_3 ={'eid': 'f.eid',\n",
    "#            '30600-0.0':'Albumin',\n",
    "#            '30610-0.0':'Alkanine_phosphatase',\n",
    "#            '30620-0.0':'Alanine_aminotransferase',\n",
    "            '30630-1.0':'APOA',\n",
    "            '30640-1.0':'APOB',\n",
    "#            '30650-0.0':'Aspartate_aminotransferase',\n",
    "#            '30680-0.0':'Calcium',\n",
    "            '30690-1.0':'Cholesterol',\n",
    "#            '30700-0.0':'Creatinine',\n",
    "            '30710-1.0':'CRP',\n",
    "#            '30730-0.0':'GGT',\n",
    "            '30740-1.0':'Glucose',\n",
    "            '30760-1.0':'HDL',\n",
    "#            '30780-0.0':'LDL',\n",
    "#            '30790-0.0':'Lipoprotein_A',\n",
    "            '30870-1.0':'Triglyceride',\n",
    "#            '30810-0.0':'Phosphate',\n",
    "#            '30820-0.0':'Rheumatoid_factor',\n",
    "#            '30880-0.0':'Uric_acid',\n",
    "#            '30670-0.0':'Urea',\n",
    "#            '30720-0.0':'Cystatin_C',\n",
    "            '30770-1.0':'IGF_1',\n",
    "#            '30890-0.0':'Vitamin_D',\n",
    "#            '30800-0.0':'Oestradiol',\n",
    "            '30850-1.0':'Testosterone',\n",
    "#            '30830-0.0':'SHBG',\n",
    "            '30750-1.0':'HbA1c',\n",
    "#            '30020-0.0':'Haemoglobin_concentration',\n",
    "#            '30080-0.0':'Platelets_count',\n",
    "#            '30000-0.0':'White_blood_cell_count',\n",
    "#            '30150-0.0':'Eosinophil_count',\n",
    "#            '30140-0.0':'Neutrophil_count'\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7858f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('/Users/marinacamacho/Desktop/Master_I/Raw_Data/ukb46359.csv', usecols = var_temp_1.keys())\n",
    "df_1 = df_1.rename(columns = var_temp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66dca745",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv('/Users/marinacamacho/Desktop/Master_I/Raw_Data/ukb46359.csv', usecols = var_temp_2.keys())\n",
    "df_2 = df_2.rename(columns = var_temp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c72e84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = pd.read_csv('/Users/marinacamacho/Desktop/Master_I/Raw_Data/ukb46359.csv', usecols = var_temp_3.keys())\n",
    "df_3 = df_3.rename(columns = var_temp_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f3ce17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['Age_full_education','Sleep_duration','Nap_during_day',\n",
    "             'Sleeplessness','Daytime_dozing/sleeping','Drinker_status',\n",
    "             'Frequency_drinking','Ever_cannabis','Ever_illicit_drug','Injury_drinking',\n",
    "             'Recommend_reduction_alcohol','Current_smoking','Past_smoking',\n",
    "             'Cooked_vegetable_intake','Salad/raw_vegetable_intake',\n",
    "             'Fresh_fruit_intake','Dried_fruit_intake','Oily_fish_intake',\n",
    "             'Non_oily_fish_intake','Processed_meat_intake','Poultry_intake',\n",
    "             'Beef_intake','Lamb/mutton_intake','Pork_intake','Cheese_intake',\n",
    "             #'Home_status',\n",
    "             #'Home_type',\n",
    "             #'Milk_type','Spread_type',\n",
    "             'Bread_intake',\n",
    "             #'Bread_type',\n",
    "             'Cereal_intake',\n",
    "             #'Cereal_type',\n",
    "             'Salt_added','Tea_intake',\n",
    "             #'Coffee_type',\n",
    "             'Water_intake','Dietary_changes_5years',\n",
    "             'Variation_diet','Belittlement_partner','Confiding_relationship',\n",
    "             'Physical_violence_partner','Sexual_interference_without_consent',\n",
    "             'Able_to_pay_rent/mortgage','Accident_life-threatening',\n",
    "             'Victim_crime','Witnessed_death','Victim_sexual_assault',\n",
    "             'Avoided_activities_1month','Disturbing_thoughts_1month',\n",
    "             'Upset_reminded_1month','Ever_depressed_1week',\n",
    "             'Longest_depression','Number_depression',\n",
    "             #'Bipolar_status',\n",
    "             #'Neuroticism_score',\n",
    "             'Ever_unenthusiastic_1week',\n",
    "             'Depressed_2weeks','Unenthusiasm_2weeks','Tenseness_2weeks',\n",
    "             'Tiredness_2weeks','Seen_doctor','Seen_sychiatrist',\n",
    "             'Ethnic',\n",
    "             'Age']\n",
    "\n",
    "# Loop over each column in the list of variables\n",
    "for col in variables:\n",
    "    # In each column, replace the values -1,-2,-3,-10,-121,-818 with np.NaN,100,np.NaN,0.5,np.NaN,np.NaN respectively.\n",
    "    # Notice that this values were coding for specific meanings in this variables, see: https://biobank.ndph.ox.ac.uk/ukb/search.cgi.\n",
    "    df_1[col] = df_1[col].replace([-1,-2,-3,-10,-121,-818],[np.NaN,100,np.NaN,0.5,np.NaN,np.NaN])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b115510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['Current_smoking']=df_1['Current_smoking'].replace([2,1], [1,2])\n",
    "df_1['Past_smoking']=df_1['Past_smoking'].replace([0,1,3,4], [4,3,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7194f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['Breastfed_baby','Adopted_child','Maternal_smoking_around_birth',\n",
    "             'Hated_family_member_child','Abused_family_child','Felt_loved_child',\n",
    "             'Sexually_molested_child','Someone_take_doctor_child']\n",
    "\n",
    "# Loop over each column in the list of variables and replace special values for appropiate ones\n",
    "for col in variables:\n",
    "    df_2[col] = df_2[col].replace([-1,-2,-3,-10,-121,-818],[np.NaN,999,np.NaN,0.5,np.NaN,np.NaN])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4e19059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map the code to its corresponding meaning\n",
    "code_meaning = {1.0: 'White',\n",
    "       2.0: 'Brown',      \n",
    "       3.0: 'WholeMealgrain',\n",
    "       4.0: 'OtherBread',\n",
    "        -1: 'Do_not_know',\n",
    "        -3: 'Prefer_not_to_answer'}\n",
    "\n",
    "# Specify the column names you want to extract\n",
    "columns_to_extract = ['Bread_type']\n",
    "\n",
    "# Create a new dataframe with only the extracted columns\n",
    "extracted_df = df_1[columns_to_extract].copy()\n",
    "\n",
    "# Create a new dataframe to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each code and meaning in the dictionary\n",
    "for code, meaning in code_meaning.items():\n",
    "    # Check if the code is positive or negative\n",
    "    is_positive = code > 0\n",
    "\n",
    "    # Create a boolean mask indicating where the code is present in the extracted columns\n",
    "    code_mask = extracted_df.isin([code])\n",
    "\n",
    "    # Count the occurrences of the code in each row\n",
    "    code_counts = code_mask.sum(axis=1)\n",
    "\n",
    "    # Create a new column with the meaning and initialize it as 1 if the code is present, else 0\n",
    "    result_df[meaning] = np.where(code_counts > 0, 1, 0)\n",
    "\n",
    "# Concatenate the result dataframe with the original dataframe\n",
    "df_1 = pd.concat([df_1, result_df], axis=1)\n",
    "\n",
    "df_1['White'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['White'])\n",
    "df_1['Brown'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Brown'])\n",
    "df_1['WholeMealgrain'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['WholeMealgrain'])\n",
    "df_1['OtherBread'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['OtherBread'])\n",
    "\n",
    "df_1['White'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['White'])\n",
    "df_1['Brown'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Brown'])\n",
    "df_1['WholeMealgrain'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['WholeMealgrain'])\n",
    "df_1['OtherBread'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['OtherBread'])\n",
    "\n",
    "del df_1['Do_not_know']\n",
    "del df_1['Prefer_not_to_answer']\n",
    "del df_1['Bread_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "691cfc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map the code to its corresponding meaning\n",
    "code_meaning = {1: 'Full_cream',\n",
    "    2: 'Semi-skimmed',\n",
    "    3: 'Skimmed',\n",
    "    4: 'Soya',\n",
    "    5: 'Other_type_of_milk',\n",
    "    6: 'Never/rarely_have_milk',\n",
    "    -1: 'Do_not_know',\n",
    "    -3: 'Prefer_not_to_answer'}\n",
    "\n",
    "# Specify the column names you want to extract\n",
    "columns_to_extract = ['Milk_type']\n",
    "\n",
    "# Create a new dataframe with only the extracted columns\n",
    "extracted_df = df_1[columns_to_extract].copy()\n",
    "\n",
    "# Create a new dataframe to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each code and meaning in the dictionary\n",
    "for code, meaning in code_meaning.items():\n",
    "    # Check if the code is positive or negative\n",
    "    is_positive = code > 0\n",
    "\n",
    "    # Create a boolean mask indicating where the code is present in the extracted columns\n",
    "    code_mask = extracted_df.isin([code])\n",
    "\n",
    "    # Count the occurrences of the code in each row\n",
    "    code_counts = code_mask.sum(axis=1)\n",
    "\n",
    "    # Create a new column with the meaning and initialize it as 1 if the code is present, else 0\n",
    "    result_df[meaning] = np.where(code_counts > 0, 1, 0)\n",
    "\n",
    "# Concatenate the result dataframe with the original dataframe\n",
    "df_1 = pd.concat([df_1, result_df], axis=1)\n",
    "\n",
    "df_1['Full_cream'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Full_cream'])\n",
    "df_1['Semi-skimmed'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Semi-skimmed'])\n",
    "df_1['Skimmed'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Skimmed'])\n",
    "df_1['Soya'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Soya'])\n",
    "df_1['Other_type_of_milk'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Other_type_of_milk'])\n",
    "df_1['Never/rarely_have_milk'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Never/rarely_have_milk'])\n",
    "\n",
    "df_1['Full_cream'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Full_cream'])\n",
    "df_1['Semi-skimmed'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Semi-skimmed'])\n",
    "df_1['Skimmed'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Skimmed'])\n",
    "df_1['Soya'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Soya'])\n",
    "df_1['Other_type_of_milk'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Other_type_of_milk'])\n",
    "df_1['Never/rarely_have_milk'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Never/rarely_have_milk'])\n",
    "\n",
    "del df_1['Do_not_know']\n",
    "del df_1['Prefer_not_to_answer']\n",
    "del df_1['Milk_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b4c6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map the code to its corresponding meaning\n",
    "code_meaning = {1: 'Butter/spreadable_butter',\n",
    "    2: 'Flora_Pro-Active/Benecol',\n",
    "    3: 'Other_type_of_spread/margarine',\n",
    "    0: 'Never/rarely_use_spread',\n",
    "    -1: 'Do_not_know',\n",
    "    -3: 'Prefer_not_to_answer'}\n",
    "\n",
    "\n",
    "# Specify the column names you want to extract\n",
    "columns_to_extract = ['Spread_type']\n",
    "\n",
    "# Create a new dataframe with only the extracted columns\n",
    "extracted_df = df_1[columns_to_extract].copy()\n",
    "\n",
    "# Create a new dataframe to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each code and meaning in the dictionary\n",
    "for code, meaning in code_meaning.items():\n",
    "    # Check if the code is positive or negative\n",
    "    is_positive = code > 0\n",
    "\n",
    "    # Create a boolean mask indicating where the code is present in the extracted columns\n",
    "    code_mask = extracted_df.isin([code])\n",
    "\n",
    "    # Count the occurrences of the code in each row\n",
    "    code_counts = code_mask.sum(axis=1)\n",
    "\n",
    "    # Create a new column with the meaning and initialize it as 1 if the code is present, else 0\n",
    "    result_df[meaning] = np.where(code_counts > 0, 1, 0)\n",
    "\n",
    "# Concatenate the result dataframe with the original dataframe\n",
    "df_1 = pd.concat([df_1, result_df], axis=1)\n",
    "\n",
    "df_1['Butter/spreadable_butter'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Butter/spreadable_butter'])\n",
    "df_1['Flora_Pro-Active/Benecol'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Flora_Pro-Active/Benecol'])\n",
    "df_1['Other_type_of_spread/margarine'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Other_type_of_spread/margarine'])\n",
    "df_1['Never/rarely_use_spread'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Never/rarely_use_spread'])\n",
    "\n",
    "df_1['Butter/spreadable_butter'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Butter/spreadable_butter'])\n",
    "df_1['Flora_Pro-Active/Benecol'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Flora_Pro-Active/Benecol'])\n",
    "df_1['Other_type_of_spread/margarine'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Other_type_of_spread/margarine'])\n",
    "df_1['Never/rarely_use_spread'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Never/rarely_use_spread'])\n",
    "\n",
    "del df_1['Do_not_know']\n",
    "del df_1['Prefer_not_to_answer']\n",
    "del df_1['Spread_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cce5e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map the code to its corresponding meaning\n",
    "code_meaning = {1: 'Bran_cereal',\n",
    "    2: 'Biscuit_cereal',\n",
    "    3: 'Oat_cereal',\n",
    "    4: 'Muesli',\n",
    "    5: 'Other_cereal',\n",
    "    -1: 'Do_not_know',\n",
    "    -3: 'Prefer_not_to_answer'}\n",
    "\n",
    "# Specify the column names you want to extract\n",
    "columns_to_extract = ['Cereal_type']\n",
    "\n",
    "# Create a new dataframe with only the extracted columns\n",
    "extracted_df = df_1[columns_to_extract].copy()\n",
    "\n",
    "# Create a new dataframe to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each code and meaning in the dictionary\n",
    "for code, meaning in code_meaning.items():\n",
    "    # Check if the code is positive or negative\n",
    "    is_positive = code > 0\n",
    "\n",
    "    # Create a boolean mask indicating where the code is present in the extracted columns\n",
    "    code_mask = extracted_df.isin([code])\n",
    "\n",
    "    # Count the occurrences of the code in each row\n",
    "    code_counts = code_mask.sum(axis=1)\n",
    "\n",
    "    # Create a new column with the meaning and initialize it as 1 if the code is present, else 0\n",
    "    result_df[meaning] = np.where(code_counts > 0, 1, 0)\n",
    "\n",
    "# Concatenate the result dataframe with the original dataframe\n",
    "df_1 = pd.concat([df_1, result_df], axis=1)\n",
    "\n",
    "df_1['Bran_cereal'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Bran_cereal'])\n",
    "df_1['Biscuit_cereal'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Biscuit_cereal'])\n",
    "df_1['Oat_cereal'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Oat_cereal'])\n",
    "df_1['Muesli'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Muesli'])\n",
    "df_1['Other_cereal'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Other_cereal'])\n",
    "\n",
    "df_1['Bran_cereal'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Bran_cereal'])\n",
    "df_1['Biscuit_cereal'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Biscuit_cereal'])\n",
    "df_1['Oat_cereal'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Oat_cereal'])\n",
    "df_1['Muesli'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Muesli'])\n",
    "df_1['Other_cereal'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Other_cereal'])\n",
    "\n",
    "del df_1['Do_not_know']\n",
    "del df_1['Prefer_not_to_answer']\n",
    "del df_1['Cereal_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "743892a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map the code to its corresponding meaning\n",
    "code_meaning = {1: 'Decaffeinated_coffee',\n",
    "    2: 'Instant_coffee',\n",
    "    3: 'Ground_coffee',\n",
    "    4: 'Other_coffee',\n",
    "    -1: 'Do_not_know',\n",
    "    -3: 'Prefer_not_to_answer'}\n",
    "\n",
    "# Specify the column names you want to extract\n",
    "columns_to_extract = ['Coffee_type']\n",
    "\n",
    "# Create a new dataframe with only the extracted columns\n",
    "extracted_df = df_1[columns_to_extract].copy()\n",
    "\n",
    "# Create a new dataframe to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each code and meaning in the dictionary\n",
    "for code, meaning in code_meaning.items():\n",
    "    # Check if the code is positive or negative\n",
    "    is_positive = code > 0\n",
    "\n",
    "    # Create a boolean mask indicating where the code is present in the extracted columns\n",
    "    code_mask = extracted_df.isin([code])\n",
    "\n",
    "    # Count the occurrences of the code in each row\n",
    "    code_counts = code_mask.sum(axis=1)\n",
    "\n",
    "    # Create a new column with the meaning and initialize it as 1 if the code is present, else 0\n",
    "    result_df[meaning] = np.where(code_counts > 0, 1, 0)\n",
    "\n",
    "# Concatenate the result dataframe with the original dataframe\n",
    "df_1 = pd.concat([df_1, result_df], axis=1)\n",
    "\n",
    "df_1['Decaffeinated_coffee'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Decaffeinated_coffee'])\n",
    "df_1['Instant_coffee'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Instant_coffee'])\n",
    "df_1['Ground_coffee'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Ground_coffee'])\n",
    "df_1['Other_coffee'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Other_coffee'])\n",
    "\n",
    "df_1['Decaffeinated_coffee'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Decaffeinated_coffee'])\n",
    "df_1['Instant_coffee'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Instant_coffee'])\n",
    "df_1['Ground_coffee'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Ground_coffee'])\n",
    "df_1['Other_coffee'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Other_coffee'])\n",
    "\n",
    "del df_1['Do_not_know']\n",
    "del df_1['Prefer_not_to_answer']\n",
    "del df_1['Coffee_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbe9cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map the code to its corresponding meaning\n",
    "code_meaning = {1: 'House/bungalow',\n",
    "    2: 'Flat/maisonette/apartment',\n",
    "    3: 'Mobile/temporary_structure',\n",
    "    4: 'Sheltered_accommodation',\n",
    "    5: 'Care_home',\n",
    "    -1: 'Do_not_know',\n",
    "    -3: 'Prefer_not_to_answer'}\n",
    "\n",
    "# Specify the column names you want to extract\n",
    "columns_to_extract = ['Home_type']\n",
    "\n",
    "# Create a new dataframe with only the extracted columns\n",
    "extracted_df = df_1[columns_to_extract].copy()\n",
    "\n",
    "# Create a new dataframe to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each code and meaning in the dictionary\n",
    "for code, meaning in code_meaning.items():\n",
    "    # Check if the code is positive or negative\n",
    "    is_positive = code > 0\n",
    "\n",
    "    # Create a boolean mask indicating where the code is present in the extracted columns\n",
    "    code_mask = extracted_df.isin([code])\n",
    "\n",
    "    # Count the occurrences of the code in each row\n",
    "    code_counts = code_mask.sum(axis=1)\n",
    "\n",
    "    # Create a new column with the meaning and initialize it as 1 if the code is present, else 0\n",
    "    result_df[meaning] = np.where(code_counts > 0, 1, 0)\n",
    "\n",
    "# Concatenate the result dataframe with the original dataframe\n",
    "df_1 = pd.concat([df_1, result_df], axis=1)\n",
    "\n",
    "df_1['House/bungalow'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['House/bungalow'])\n",
    "df_1['Flat/maisonette/apartment'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Flat/maisonette/apartment'])\n",
    "df_1['Mobile/temporary_structure'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Mobile/temporary_structure'])\n",
    "df_1['Sheltered_accommodation'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Sheltered_accommodation'])\n",
    "df_1['Care_home'] = np.where(df_1['Do_not_know'] == 1, np.nan, df_1['Care_home'])\n",
    "\n",
    "df_1['House/bungalow'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['House/bungalow'])\n",
    "df_1['Flat/maisonette/apartment'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Flat/maisonette/apartment'])\n",
    "df_1['Sheltered_accommodation/temporary_structure'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Mobile/temporary_structure'])\n",
    "df_1['Muesli'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Sheltered_accommodation'])\n",
    "df_1['Care_home'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.nan, df_1['Care_home'])\n",
    "\n",
    "del df_1['Do_not_know']\n",
    "del df_1['Prefer_not_to_answer']\n",
    "del df_1['Home_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1c30f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map the code to its corresponding meaning\n",
    "code_meaning = {\n",
    "    -7.0: 'None_of_the_above',\n",
    "    -3.0: 'Prefer_not_to_answer',\n",
    "    1.0: 'Own_outright',\n",
    "    2.0: 'Own_mortgage', \n",
    "    3.0: 'Rent_local',\n",
    "    4.0: 'Rent_private',      \n",
    "    5.0: 'Pay_rent_mortgage',  \n",
    "    6.0: 'Rent_free',      \n",
    "}\n",
    "\n",
    "# Specify the column names you want to extract\n",
    "columns_to_extract = ['Home_status']\n",
    "\n",
    "# Create a new dataframe with only the extracted columns\n",
    "extracted_df = df_1[columns_to_extract].copy()\n",
    "\n",
    "# Create a new dataframe to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each code and meaning in the dictionary\n",
    "for code, meaning in code_meaning.items():\n",
    "\n",
    "    # Create a boolean mask indicating where the code is present in the extracted columns\n",
    "    code_mask = extracted_df.isin([code])\n",
    "\n",
    "    # Count the occurrences of the code in each row\n",
    "    code_counts = code_mask.sum(axis=1)\n",
    "\n",
    "    # Create a new column with the meaning and initialize it as 1 if the code is present, else 0\n",
    "    result_df[meaning] = np.where(code_counts > 0, 1, 0)\n",
    "\n",
    "# Concatenate the result dataframe with the original dataframe\n",
    "df_1 = pd.concat([df_1, result_df], axis=1)\n",
    "\n",
    "df_1['Own_outright'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.NaN, df_1['Own_outright'])\n",
    "df_1['Own_mortgage'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.NaN, df_1['Own_mortgage'])\n",
    "df_1['Rent_local'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.NaN, df_1['Rent_local'])\n",
    "df_1['Rent_private'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.NaN, df_1['Rent_private'])\n",
    "df_1['Pay_rent_mortgage'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.NaN, df_1['Pay_rent_mortgage'])\n",
    "df_1['Rent_free'] = np.where(df_1['Prefer_not_to_answer'] == 1, np.NaN, df_1['Rent_free'])\n",
    "\n",
    "del df_1['None_of_the_above']\n",
    "del df_1['Prefer_not_to_answer']\n",
    "del df_1['Home_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c358df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map the code to its corresponding meaning\n",
    "code_meaning = {0: 'No_Bipolar_Depression',\n",
    "    1: 'Bipolar_I_Disorder',\n",
    "    2: 'Bipolar_II_Disorder',\n",
    "    3: 'Recurrent_severe_depression',\n",
    "    4: 'Recurrent_moderate_depression',\n",
    "    5: 'Single_depression_episode'}\n",
    "\n",
    "# Specify the column names you want to extract\n",
    "columns_to_extract = ['Bipolar_status']\n",
    "\n",
    "# Create a new dataframe with only the extracted columns\n",
    "extracted_df = df_1[columns_to_extract].copy()\n",
    "\n",
    "# Create a new dataframe to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each code and meaning in the dictionary\n",
    "for code, meaning in code_meaning.items():\n",
    "    # Check if the code is positive or negative\n",
    "    is_positive = code > 0\n",
    "\n",
    "    # Create a boolean mask indicating where the code is present in the extracted columns\n",
    "    code_mask = extracted_df.isin([code])\n",
    "\n",
    "    # Count the occurrences of the code in each row\n",
    "    code_counts = code_mask.sum(axis=1)\n",
    "\n",
    "    # Create a new column with the meaning and initialize it as 1 if the code is present, else 0\n",
    "    result_df[meaning] = np.where(code_counts > 0, 1, 0)\n",
    "\n",
    "# Concatenate the result dataframe with the original dataframe\n",
    "df_1 = pd.concat([df_1, result_df], axis=1)\n",
    "\n",
    "del df_1['No_Bipolar_Depression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fe6ea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'6138-0.0': 'Qualifications'\n",
    "#'6142-0.0': 'Employment_status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43729c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map the code to its corresponding meaning\n",
    "code_meaning = {\n",
    "    -7.0: 'None_of_the_above',\n",
    "    -3.0: 'Prefer_not_to_answer',\n",
    "    1.0: 'University',  \n",
    "    2.0: 'A/AS',\n",
    "    3.0: 'O/GCSE',\n",
    "    4.0: 'CSE',\n",
    "    5.0: 'NVQ/HND/HNC',\n",
    "    6.0: 'Professional'\n",
    "}\n",
    "\n",
    "# Specify the column names you want to extract\n",
    "columns_to_extract = ['Qualifications_0','Qualifications_1','Qualifications_2',\n",
    "                      'Qualifications_3','Qualifications_4','Qualifications_5']\n",
    "\n",
    "# Create a new dataframe with only the extracted columns\n",
    "extracted_df = df_1[columns_to_extract].copy()\n",
    "\n",
    "# Create a new dataframe to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each code and meaning in the dictionary\n",
    "for code, meaning in code_meaning.items():\n",
    "    \n",
    "    # Create a boolean mask indicating where the code is present in the extracted columns\n",
    "    code_mask = extracted_df.isin([code])\n",
    "\n",
    "    # Count the occurrences of the code in each row\n",
    "    code_counts = code_mask.sum(axis=1)\n",
    "\n",
    "    # Create a new column with the meaning and initialize it as 1 if the code is present, else 0\n",
    "    result_df[meaning] = np.where(code_counts > 0, 1, 0)\n",
    "\n",
    "# Concatenate the result dataframe with the original dataframe\n",
    "df_1 = pd.concat([df_1, result_df], axis=1)\n",
    "\n",
    "df_1['University'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['University'])\n",
    "df_1['A/AS'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['A/AS'])\n",
    "df_1['O/GCSE'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['O/GCSE'])\n",
    "df_1['CSE'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['CSE'])\n",
    "df_1['NVQ/HND/HNC'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['NVQ/HND/HNC'])\n",
    "df_1['Professional'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['Professional'])\n",
    "\n",
    "del df_1['Qualifications_0']\n",
    "del df_1['Qualifications_1']\n",
    "del df_1['Qualifications_2']\n",
    "del df_1['Qualifications_3']\n",
    "del df_1['Qualifications_4']\n",
    "del df_1['Qualifications_5']\n",
    "\n",
    "del df_1['None_of_the_above']\n",
    "del df_1['Prefer_not_to_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecbd220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to map the code to its corresponding meaning\n",
    "code_meaning = {\n",
    "    -7.0: 'None_of_the_above',\n",
    "    -3.0: 'Prefer_not_to_answer',\n",
    "    1.0: 'Paid_employment',\n",
    "    2.0: 'Retired', \n",
    "    3.0: 'Looking_after_home',\n",
    "    4.0: 'Unable_to_work',      \n",
    "    5.0: 'Unemployed',  \n",
    "    6.0: 'Unpaid_work',      \n",
    "    7.0: 'Student'\n",
    "}\n",
    "\n",
    "# Specify the column names you want to extract\n",
    "columns_to_extract = ['Employment_status_0','Employment_status_1','Employment_status_2',\n",
    "                      'Employment_status_3','Employment_status_4','Employment_status_5',\n",
    "                      'Employment_status_6']\n",
    "\n",
    "# Create a new dataframe with only the extracted columns\n",
    "extracted_df = df_1[columns_to_extract].copy()\n",
    "\n",
    "# Create a new dataframe to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each code and meaning in the dictionary\n",
    "for code, meaning in code_meaning.items():\n",
    "\n",
    "    # Create a boolean mask indicating where the code is present in the extracted columns\n",
    "    code_mask = extracted_df.isin([code])\n",
    "\n",
    "    # Count the occurrences of the code in each row\n",
    "    code_counts = code_mask.sum(axis=1)\n",
    "\n",
    "    # Create a new column with the meaning and initialize it as 1 if the code is present, else 0\n",
    "    result_df[meaning] = np.where(code_counts > 0, 1, 0)\n",
    "\n",
    "# Concatenate the result dataframe with the original dataframe\n",
    "df_1 = pd.concat([df_1, result_df], axis=1)\n",
    "\n",
    "df_1['Paid_employment'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['Paid_employment'])\n",
    "df_1['Retired'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['Retired'])\n",
    "df_1['Looking_after_home'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['Looking_after_home'])\n",
    "df_1['Unable_to_work'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['Unable_to_work'])\n",
    "df_1['Unemployed'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['Unemployed'])\n",
    "df_1['Unpaid_work'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['Unpaid_work'])\n",
    "df_1['Student'] = np.where(df_1['None_of_the_above'] == 1, 0, df_1['Student'])\n",
    "  \n",
    "del df_1['Employment_status_0']\n",
    "del df_1['Employment_status_1']\n",
    "del df_1['Employment_status_2']\n",
    "del df_1['Employment_status_3']\n",
    "del df_1['Employment_status_4']\n",
    "del df_1['Employment_status_5']\n",
    "del df_1['Employment_status_6']\n",
    "\n",
    "del df_1['None_of_the_above']\n",
    "del df_1['Prefer_not_to_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10aa3981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(502481, 118)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3456b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(502481, 9)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "716410d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(502481, 11)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c10d4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_with_negatives_1 = df_1.columns[df_1.lt(0).any()]; columns_with_negatives_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "860c11bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_with_negatives_2 = df_2.columns[df_2.lt(0).any()]; columns_with_negatives_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97667c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_with_negatives_3 = df_3.columns[df_3.lt(0).any()]; columns_with_negatives_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b76612d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "384f6122",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.to_csv(r'/Users/marinacamacho/Desktop/Master_I/Raw_Data/Time_1/ukb46359_clean_external.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc953d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.to_csv(r'/Users/marinacamacho/Desktop/Master_I/Raw_Data/Time_1/ukb46359_clean_internal.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "466918d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.to_csv(r'/Users/marinacamacho/Desktop/Master_I/Raw_Data/Time_1/ukb46359_clean_earlycause.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691a732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
